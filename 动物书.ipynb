{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "#chapter 1\n",
    "#向量 矩阵 数组\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_row = np.array([1,2,3])\n",
    "vector_column = np.array([[1],[2],[3]])\n",
    "matrix = np.array([[1,2],\n",
    "                  [1,2],\n",
    "                  [1,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vector_column)\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = np.array([[0,0],\n",
    "                  [0,1],\n",
    "                  [3,0]])\n",
    "\n",
    "#创建一个压缩的稀疏行(csr)矩阵\n",
    "matrix_sparse = sparse.csr_matrix(matrix)\n",
    "print(matrix_sparse)\n",
    "#//仅表示非零元"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = np.array([1,2,3,4,5,6])\n",
    "matrix = np.array([1,2,3,4,5,6,7,8,9]).reshape(3,3)\n",
    "print(vector[:3])\n",
    "print(matrix[:,1:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#向量化函数\n",
    "add_100 = lambda i: i+100\n",
    "vectorized_add_100 = np.vectorize(add_100)\n",
    "print(vectorized_add_100(matrix))\n",
    "print(matrix+100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(matrix,axis=1)#找出每一行最大的元素\n",
    "np.max(matrix,axis=0)#找出每一列最大的元素\n",
    "np.mean(matrix)\n",
    "np.var(matrix)\n",
    "np.std(matrix)\n",
    "#对于均值方差和标准差同样有对行或列求相应的值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(matrix.reshape(-1,1))\n",
    "#reshape -1表示待填充，但前提为另一参数能够整除size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#转置\n",
    "print(matrix.T)\n",
    "#一维展开\n",
    "print(matrix.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵的秩\n",
    "matrix = np.array([1,1,1,1,1,10,1,1,15]).reshape(3,3)\n",
    "print(np.linalg.matrix_rank(matrix))\n",
    "#行列式\n",
    "print(np.linalg.det(matrix))\n",
    "#获取对角元素\n",
    "print(matrix.diagonal())\n",
    "#对角线向上偏移1的元素\n",
    "print(matrix)\n",
    "print(matrix.diagonal(1))\n",
    "print(matrix.diagonal(offset=-1))\n",
    "\n",
    "#矩阵的迹\n",
    "print(matrix.trace())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特征值与特征向量\n",
    "matrix = np.array([1,-1,3,1,1,6,3,8,9]).reshape(3,3)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(matrix)\n",
    "print(eigenvalues,'\\n',eigenvectors)\n",
    "#列向量为特征向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#向量点积\n",
    "vector_a = np.array([1,2,3])\n",
    "vector_b = np.array([4,5,6])\n",
    "#对应元素相乘相加\n",
    "print(np.dot(vector_a,vector_b))\n",
    "vector_a@vector_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵相加减\n",
    "matrix_a = np.array([1,1,1,1,1,1,1,1,2]).reshape(3,3)\n",
    "matrix_b = np.array([1,3,1,1,3,1,1,3,8]).reshape(3,3)\n",
    "print(np.add(matrix_a,matrix_b))\n",
    "print(np.subtract(matrix_a,matrix_b))\n",
    "print(matrix_a+matrix_b)\n",
    "print(matrix_a-matrix_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵相乘\n",
    "matrix_a = np.array([1,1,1,2]).reshape(2,2)\n",
    "matrix_b = np.array([1,3,1,2]).reshape(2,2)\n",
    "#矩阵乘法\n",
    "print(np.dot(matrix_a,matrix_b))\n",
    "print(matrix_a@matrix_b)\n",
    "#对应位置相乘\n",
    "print(matrix_a*matrix_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#矩阵的逆\n",
    "matrix = np.array([1,4,2,5]).reshape(2,2)\n",
    "print(np.linalg.inv(matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#随机数\n",
    "np.random.seed(0)#随机种子\n",
    "np.random.random(3)#产生三个0.0到1.0的随机数\n",
    "np.random.randint(0,11,3)#产生三个1到10的随机数\n",
    "np.random.normal(0.0,1.0,3)#均值为0，标准差为1.0的正态分布中抽3个数\n",
    "np.random.logistic(0.0,1.0,3)#均值为0，散布程度为1.0的logistic分布中抽3个数\n",
    "np.random.uniform(1.0,2.0,3)#1.0到2.0的均匀分布抽3个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "#chapter 2\n",
    "#加载数据\n",
    "\n",
    "#内置数据\n",
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "features = digits.data\n",
    "\n",
    "target = digits.target\n",
    "\n",
    "features[0]\n",
    "\n",
    "# load_boston #波士顿房价观测值，用于研究回归\n",
    "# load_iris #鸢尾花尺寸观测值，用于研究分类算法\n",
    "# load_digits #手写数字图片的观测值，用于研究图像分类算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#创建仿真数据集\n",
    "from sklearn.datasets import make_regression#随机产生回归模型的数据\n",
    "features, target, coeffucuents = make_regression(n_samples = 100,#样本个数\n",
    "                                                 n_features = 3,#自变量个数\n",
    "                                                 n_informative = 3,#有效变量个数\n",
    "                                                 n_targets = 1,#因变量个数\n",
    "                                                 noise = 0.0,#噪声\n",
    "                                                 coef = True,#是否返回回归系数\n",
    "                                                 random_state = 1)#随机状态，类似于随机种子\n",
    "print('Feature Matrix\\n',features[:3])#特征矩阵\n",
    "print('Target Vector\\n',target[:3])#目标向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification#随机产生分类模型的数据\n",
    "\n",
    "features, target =make_classification(n_samples = 100,\n",
    "                                      n_features = 3,\n",
    "                                      n_informative = 3,\n",
    "                                      n_redundant = 0,#冗余信息\n",
    "                                      n_classes = 2,\n",
    "                                      weights = [.25, .75],#类别的权重\n",
    "                                      random_state = 1)\n",
    "print('Feature Matrix\\n',features[:3])\n",
    "print('Target Vector\\n',target[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs#随机产生聚类模型的数据\n",
    "\n",
    "features, target = make_blobs(n_samples = 100,\n",
    "                              n_features = 2,\n",
    "                              centers = 3,#生成的聚类数\n",
    "                              cluster_std = 0.5,#每个簇的标准差\n",
    "                              shuffle = True,#将数据进行洗乱\n",
    "                              random_state = 1)\n",
    "\n",
    "print('Feature Matrix\\n', features[:3])\n",
    "print('Target Vector\\n',target[:3])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(features[:,0],features[:,1],c=target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#加载文件\n",
    "import pandas as pd\n",
    "#csv\n",
    "url = 'https://tinyurl.com/simulated_data'\n",
    "\n",
    "dataframe = pd.read_csv(url)\n",
    "dataframe.head(2)\n",
    "\n",
    "#excel\n",
    "url = 'https://tinyurl.com/simulated_excel'\n",
    "\n",
    "dataframe = pd.read_excel(url, sheetname=0, header=1)\n",
    "#sheetname 数据表 可以为数字（从0开始），也可以是表名\n",
    "dataframe.head(2)\n",
    "\n",
    "url = 'https://tinyurl.com/simulated_json0'\n",
    "dataframe = pd.read_json(url, orint='columns')\n",
    "dataframe.head(2)\n",
    "\n",
    "import sqlalchemy import create_engine\n",
    "\n",
    "database_connection = create_engine('sqlite:///sample.db')\n",
    "dataframe = pd.read_sql_query('SELECT * FROM data', database_connection)\n",
    "dataframe.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chapter 3 数据整理\n",
    "##################\n",
    "import pandas as pd\n",
    "\n",
    "dataframe = pd.read_csv('titanic.csv')\n",
    "\n",
    "dataframe.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame()\n",
    "dataframe['Name'] = ['Jacky Jackson','Steven Stevenson']\n",
    "dataframe['Age'] = [38, 25]\n",
    "dataframe['Driver'] = [True, False]\n",
    "\n",
    "new_person = pd.Series(['Molly Mooney', 40, True], index=['Name','Age','Driver'])#初始化一个一维数组\n",
    "dataframe.append(new_person, ignore_index=True)#加入一行 ignore_index默认为false，如果为True则不使用inde标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#描述数据\n",
    "dataframe = pd.read_csv('titanic.csv')\n",
    "dataframe.head(2)\n",
    "dataframe.tail(2)\n",
    "print(dataframe.shape)\n",
    "print(dataframe.describe())#描述性统计量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#浏览数据\n",
    "#使用loc或者iloc选择一个或多个数据，也可以选择一行或多行数据\n",
    "#loc:当数据的索引是一个标签时\n",
    "#iloc:根据行号来查找数据\n",
    "dataframe.iloc[0]\n",
    "dataframe.iloc[:4]#到某一行为止的所有行\n",
    "\n",
    "#设置索引\n",
    "dataframe = dataframe.set_index(dataframe['Name'])\n",
    "dataframe.loc['Allen, Miss Elisabeth Walton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#根据条件语句来选择行\n",
    "dataframe = pd.read_csv('titanic.csv')\n",
    "\n",
    "dataframe[dataframe['Sex'] == 'female'].head(2)\n",
    "\n",
    "dataframe[(dataframe['Sex'] == 'female')&(dataframe['Age'] >= 65)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#替换值\n",
    "dataframe['Sex'].replace(\"female\",\"Woman\").head(2)\n",
    "dataframe['Sex'].replace([\"female\",\"male\"],[\"Woman\",\"Man\"]).head(5)\n",
    "dataframe.replace(1,\"One\").head(2)\n",
    "dataframe.replace(r\"1st\", \"First\", regex=True).head(2)#正则表达式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#重命名列\n",
    "import pandas as pd\n",
    "dataframe = pd.read_csv('titanic.csv')\n",
    "\n",
    "print(dataframe.rename(columns={'PClass': 'Passeger Class'}).head(2))\n",
    "print(dataframe.rename(columns={'PClass': 'Passenger Class', 'Sex':'Gender'}).head(2))\n",
    "\n",
    "import collections#以旧列名为键、空字符串为值，创建一个字典\n",
    "column_names = collections.defaultdict(str)\n",
    "for name in dataframe.columns:\n",
    "    column_names[name]\n",
    "\n",
    "column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算最值、总和、平均值、计数值\n",
    "\n",
    "dataframe = pd.read_csv('titanic.csv')\n",
    "\n",
    "print('Maximum:',dataframe['Age'].max())\n",
    "print('Minimum:',dataframe['Age'].min())\n",
    "print('Mean:',dataframe['Age'].mean())\n",
    "print('Sum:',dataframe['Age'].sum())\n",
    "print('Count:',dataframe['Age'].count())\n",
    "#方差var、标准差std、峰态kurt、偏态skew、平均值标准误差sem、众数mode、中位数median\n",
    "dataframe.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#查找唯一值\n",
    "print(dataframe['Sex'].unique())#筛选出唯一值\n",
    "print(dataframe['Sex'].value_counts())#查看出现次数\n",
    "\n",
    "dataframe['PClass'].value_counts()#*为异常值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#处理缺失值\n",
    "dataframe[dataframe['Age'].isnull()].head(2)#筛选缺失值\n",
    "\n",
    "#dataframe['Sex'] = dataframe['Sex'].replace('male', NaN)\n",
    "import numpy as np\n",
    "dataframe['Sex'] = dataframe['Sex'].replace('male', np.nan)\n",
    "\n",
    "dataframe = pd.read_csv('titanic.csv', na_values=[np.nan, 'NONE', -999])#加载数据，设置缺失值\n",
    "dataframe.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#删除一列\n",
    "#并不修改本身，但返回修改后的值\n",
    "dataframe = pd.read_csv('titanic.csv')\n",
    "dataframe.drop(['Age', 'Sex'], axis=1).head(2)#删除多列\n",
    "dataframe.drop(['Age'],axis=1).head(2)#删除一列\n",
    "dataframe.drop(dataframe.columns[1],axis=1).head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除一行\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"titanic.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[dataframe['Sex'] != 'male'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[dataframe['Name'] != 'Allison, Miss Hellen Loraine'].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe[dataframe.index != 0].head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除重复行\n",
    "dataframe.drop_duplicates().head(2)\n",
    "# drop_duplicates()只删除完美匹配的行\n",
    "dataframe.drop_duplicates(subset=['Sex'])#优先保留先出现的行\n",
    "dataframe.drop_duplicates(subset=['Sex'],keep = 'last')#优先保留后出现的行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 根据值对行分组\n",
    "dataframe.groupby(\"Sex\")#.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.groupby(\"Sex\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.groupby(\"Survived\")['Name'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.groupby([\"Sex\",\"Survived\"])['Age'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 按时间段进行分组\n",
    "import numpy as np\n",
    "time_index = pd.date_range('06/06/2017',periods=100000,freq='30S')#创建日期范围\n",
    "dataframe = pd.DataFrame(index = time_index)#创建数据帧\n",
    "dataframe['Sale_Amount']=np.random.randint(1,10,100000)#创建一列随机变量\n",
    "dataframe.resample('W').sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.resample('2W').mean()#按两周分组，计算平均值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe.resample(\"M\").count()#按月分组，计算行数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 遍历一个列的数据\n",
    "dataframe = pd.read_csv('titanic.csv')\n",
    "\n",
    "for name in dataframe['Name'][0:2]:\n",
    "    print(name.upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[name.upper() for name in dataframe[\"Name\"][0:2]]#以大写的形式打印前两行的名字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对一列的所有元素应用某个函数\n",
    "def uppercase(x):\n",
    "    return x.upper()\n",
    "\n",
    "dataframe['Name'].apply(uppercase)[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对所有分组应用一个函数\n",
    "dataframe.groupby('Sex').apply(lambda x: x.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 连接多个数据帧\n",
    "data_a = {'id':[\"1\",\"2\",\"3\"],\n",
    "          'first':['Alex','Amy','Allen'],\n",
    "          'last':['Anderson','Ackerman','Ali']}\n",
    "dataframe_a = pd.DataFrame(data_a,columns=['id','first','last'])\n",
    "\n",
    "data_b = {'id':['4','5','6'],\n",
    "          'first':['Billy',\"Brian\",'Bran'],\n",
    "          'last':['Bonder','Black','Balwner']}\n",
    "dataframe_b = pd.DataFrame(data_b,columns=['id','first','last'])\n",
    "\n",
    "pd.concat([dataframe_a,dataframe_b],axis = 0)#沿着行的方向链接两个数据帧\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([dataframe_a,dataframe_b],axis=1)#在列的方向上链接两个数据帧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 合并两个数据帧\n",
    "employee_data = {'employee_id':['1','2','3','4'],\n",
    "                'name':['Amy Jones','Allen Keys','Alice Bees','Tim Horton']}\n",
    "dataframe_employees = pd.DataFrame(employee_data,columns=['employee_id','name'])\n",
    "\n",
    "sales_data = {'employee_id':['3','4','5','6'],\n",
    "             'total_sales':[23456,2512,2345,1455]}\n",
    "dataframe_sales = pd.DataFrame(sales_data,columns = ['employee_id','total_sales'])\n",
    "\n",
    "pd.merge(dataframe_employees,dataframe_sales,on='employee_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(dataframe_employees,dataframe_sales,on='employee_id',how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.merge(dataframe_employees,dataframe_sales,on='employee_id',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.merge(dataframe_employees,dataframe_sales,left_on='employee_id',right_on='employee_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "#处理数值型数据\n",
    "# 特征的缩放\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "feature = np.array([[-500.5],[-100.1],[0],[100.1],[900.0]])\n",
    "minmax_sxale = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "scaled_feature = minmax_sxale.fit_transform(feature)\n",
    "scaled_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特征标准化\n",
    "x = np.array([[-1000.1],[-200.2],[500.5],[600.6],[9000.9]])\n",
    "scaler = preprocessing.StandardScaler()\n",
    "standardized = scaler.fit_transform(x)\n",
    "print(standardized)\n",
    "print(\"Mean\",round(standardized.mean()))\n",
    "print(\"Standard deviation\",round(standardized.std()))\n",
    "# 中位数和四分位数间距进行放缩\n",
    "robust_scaler = preprocessing.RobustScaler()\n",
    "robust_scaler.fit_transform(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 归一化观察值\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "feature = np.array([[0.5,0.5],[1.1,3.4],[1.5,20.2],[1.63,34.3],[10.9,3.3]])\n",
    "normalizer = Normalizer(norm=\"l2\")#L2范数——欧式范数\n",
    "\n",
    "normalizer.transform(feature)\n",
    "\n",
    "# 转换特征矩阵\n",
    "feature_l2_norm = Normalizer(norm='l2').transform(feature)\n",
    "\n",
    "# 曼哈顿范数——L1范数（绝对值求和）\n",
    "feature_l1_norm = Normalizer(norm=\"l1\").transform(feature)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成多项式和交互特征\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "features = np.array([[2,3],[2,3],[2,3]])\n",
    "\n",
    "ploynomial_interaction = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# 创建多项式特征\n",
    "ploynomial_interaction.fit_transform(features)\n",
    "# degree 参数决定了多项式的最高阶数\n",
    "# 为3时\n",
    "# x1,x2,x1^2,x2^2,x1^3,x2^3\n",
    "# include_bias为交互特征即交互项\n",
    "# 为true时强制创建出来的特征只包含交互特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 转换特征\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "features = np.array([[2,3],[2,3],[2,3]])\n",
    "\n",
    "# 定义一个简单的函数\n",
    "def add_ten(x):\n",
    "    return x+10\n",
    "ten_transformer = FunctionTransformer(add_ten)\n",
    "\n",
    "ten_transformer.transform(features)\n",
    "\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(features,columns=[\"feature_1\",\"feature_2\"])\n",
    "df.apply(add_ten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 识别异常值\n",
    "import numpy as np\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "features , _ = make_blobs(n_samples = 10,n_features=2,centers=1,random_state=1)\n",
    "\n",
    "# 将第一个观察值的值替换为极端值\n",
    "features[0,0]=10000\n",
    "features[0,1]=10000\n",
    "\n",
    "# 创建识别器\n",
    "outlier_detector = EllipticEnvelope(contamination=.1)#异常值在观察值中的比例为0.1\n",
    "# 拟合识别器\n",
    "outlier_detector.fit(features)\n",
    "# 预测异常值\n",
    "outlier_detector.predict(features)\n",
    "\n",
    "\n",
    "# 查看某些特征，并使用四分位差（IQR）来识别这些特征的极端值\n",
    "feature = features[:,0]\n",
    "# 创建一个函数来返回异常值的下标\n",
    "def indicies_if_outliers(x):\n",
    "    q1,q3=np.percentile(x,[25,75])\n",
    "    iqr = q3-q1\n",
    "    lower_bound = q1 - (iqr*1.5)\n",
    "    upper_bound = q3 + (iqr*1.5)\n",
    "    return np.where((x>upper_bound)|(x<lower_bound))\n",
    "\n",
    "# 执行函数\n",
    "print(indicies_if_outliers(feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理异常值\n",
    "import pandas as pd\n",
    "\n",
    "houses = pd.DataFrame()\n",
    "houses[\"Price\"] = [534433,392333,293222,4322032]\n",
    "houses['Bathrooms'] = [2,3.5,2,116]\n",
    "houses['Square_Feet'] = [1500,2500,1500,48000]\n",
    "\n",
    "import numpy as np\n",
    "houses[\"Outlier\"] = np.where(houses['Bathrooms']<20,0,1)\n",
    "\n",
    "# 对特征取对数值\n",
    "houses[\"Log_Of_Square_Feet\"] = [np.log(x) for x in houses[\"Square_Feet\"]]\n",
    "houses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将特征离散化\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "age = np.array([[6],[12],[20],[36],[65]])\n",
    "\n",
    "# 创建二值化器\n",
    "binarizer = Binarizer(18)\n",
    "# 转化特征\n",
    "binarizer.fit_transform(age)#大于18为1，小于为0\n",
    "\n",
    "# 多个阈值将数值型特征离散化\n",
    "np.digitize(age,bins=[20,30,64])#bins为左闭右开，即第一个区间不包括20，可以设置right=True来改变\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用聚类的方式将观察值分组\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "features,_ = make_blobs(n_samples=50,n_features=2,centers=3,random_state=1)\n",
    "\n",
    "dataframe = pd.DataFrame(features,columns=[\"feature_1\",\"feature_2\"])\n",
    "\n",
    "clusterer = KMeans(3,random_state=0)\n",
    "clusterer.fit(features)\n",
    "dataframe[\"group\"]=clusterer.predict(features)\n",
    "dataframe.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除带有缺失值的观察值\n",
    "features = np.array([[1.1,11.1],[2.2,22.2],[3.3,33.3],[4.4,44.4],[np.nan,55]])\n",
    "# 只保留没有（用~来表示）缺失值的观察值\n",
    "features[~np.isnan(features).any(axis=1)]\n",
    "\n",
    "# 用pandas\n",
    "dataframe = pd.DataFrame(features,columns=[\"feature_1\",\"feature_2\"])\n",
    "dataframe.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于fancyimpute安装不了 本节不以演示\n",
    "# from fancyimpute import KNN\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.datasets import make_blobs\n",
    "\n",
    "# features,_ = make_blobs(n_samples=1000,n_features=2,random_state=1)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# standardized_features = scaler.fit_transform(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充缺失值（使用scikit-learn中的Imputer模块）\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "features,_ = make_blobs(n_samples=1000,n_features=2,random_state=1)\n",
    "\n",
    "mean_imputer = Imputer(strategy=\"mean\",axis=0)\n",
    "\n",
    "features_mean_imputer = mean_imputer.fit_transform(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# 第五章处理分类数据\n",
    "##############################\n",
    "\n",
    "# 对nominal型分类特征编码（无内部顺序）\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer, MultiLabelBinarizer\n",
    "\n",
    "feature = np.array([[\"Texas\"],[\"California\"],[\"Texas\"],[\"Delaware\"],[\"Texas\"]])\n",
    "\n",
    "one_hot = LabelBinarizer()\n",
    "one_hot.fit_transform(feature)\n",
    "# 输出分类\n",
    "one_hot.classes_\n",
    "\n",
    "# 对one_hot编码进行逆转化\n",
    "one_hot.inverse_transform(one_hot.fit_transform(feature))\n",
    "\n",
    "# pandas进行独热编码\n",
    "import pandas as pd\n",
    "pd.get_dummies(feature[:,0])\n",
    "\n",
    "# 多个分类的特征\n",
    "multiclass_feature = [(\"Texas\",\"Florida\"),(\"California\",\"Alabama\"),(\"Texas\",\"Florida\"),(\"Dekware\",\"Florida\"),(\"Texas\",\"Alabama\")]\n",
    "one_hot_multiclass = MultiLabelBinarizer()\n",
    "\n",
    "one_hot_multiclass.fit_transform(multiclass_feature)\n",
    "\n",
    "one_hot_multiclass.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对ordinal分类特征编码（有内部顺序）\n",
    "import pandas as pd\n",
    "\n",
    "dataframe = pd.DataFrame({\"Score\":[\"Low\",\"Low\",\"Medium\",\"Medium\",\"High\"]})\n",
    "\n",
    "scale_mapper = {\"Low\":1,\"Medium\":2,\"High\":3}\n",
    "dataframe[\"Score\"].replace(scale_mapper)\n",
    "# 要注意间隔要相等，以及映射的数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对特征字典编码\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "data_dict = [{\"Red\":2,\"Blue\":4},{\"Red\":4,\"Blue\":3},{\"Red\":1,\"Yellow\":2},{\"Red\":2,\"Yellow\":2}]\n",
    "\n",
    "dictvecirizer = DictVectorizer(sparse=False)# 强制DictVectorizer输出稠密矩阵\n",
    "features = dictvecirizer.fit_transform(data_dict)\n",
    "print(features)\n",
    "\n",
    "# 获取特征名字\n",
    "feature_names = dictvecirizer.get_feature_names()\n",
    "print(feature_names)\n",
    "\n",
    "import pandas as pd\n",
    "print(pd.DataFrame(features,columns=feature_names))\n",
    "\n",
    "doc_1_word_count = {\"Red\":2,\"Blue\":4}\n",
    "doc_2_word_count = {\"Red\":4,\"Blue\":3}\n",
    "doc_3_word_count = {\"Red\":1,\"Yellow\":2}\n",
    "doc_4_word_count = {\"Red\":2,\"Yellow\":2}\n",
    "\n",
    "doc_word_counts = [doc_1_word_count,doc_2_word_count,doc_3_word_count,doc_4_word_count]\n",
    "# 将词频字典列表转换成特征矩阵\n",
    "dictvecirizer.fit_transform(doc_word_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 填充缺失的分类值\n",
    "# KNN预测\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X = np.array([[0,2.10,1.45],[1,1.18,1.33],[0,1.22,1.27],[1,-0.21,-0.22]])\n",
    "X_with_nan = np.array([[np.nan,.87,1.31],[np.nan,-.67,-.22]])\n",
    "\n",
    "clf = KNeighborsClassifier(3,weights=\"distance\")\n",
    "trained_model = clf.fit(X[:,1:],X[:,0])\n",
    "\n",
    "imputed_values = trained_model.predict(X_with_nan[:,1:])\n",
    "# 将所预测的分类和他们的其他特征连接起来\n",
    "X_with_imputed = np.hstack((imputed_values.reshape(-1,1),X_with_nan[:,1:]))\n",
    "# 连接两个特征矩阵\n",
    "print(np.vstack((X_with_imputed,X)))\n",
    "\n",
    "# 众数\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "X_complete = np.vstack((X_with_nan,X))\n",
    "imputer = Imputer(strategy=\"most_frequent\",axis=0)\n",
    "print(imputer.fit_transform(X_complete))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理不均衡数据\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "\n",
    "features = iris.data\n",
    "\n",
    "target = iris.target\n",
    "# 移除前40个观察值\n",
    "features = features[40:,:]\n",
    "target = target[40:]\n",
    "# 创建二元目标向量来识别观察值是否为类别0\n",
    "target = np.where((target == 0),0,1)\n",
    "\n",
    "target\n",
    "\n",
    "# 创建权重\n",
    "weights = {0:.9,1:.1}\n",
    "# 创建带权重的随机森林分类器\n",
    "RandomForestClassifier(class_weight = weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个带均衡分类权重的随机森林分类器\n",
    "RandomForestClassifier(class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 给每个分类的观察值打标签\n",
    "i_class0 = np.where(target == 0)[0]\n",
    "i_class1 = np.where(target == 1)[0]\n",
    "# 确定每个分类的观察值数量\n",
    "n_class0 = len(i_class0)\n",
    "n_class1 = len(i_class1)\n",
    "# 上采样：重复抽取少的那一部分\n",
    "# 下采样：抽取多的那一部分\n",
    "# 对于每个分类为0的观察值，从分类为1的数据中进行无放回2的随机采样\n",
    "i_class1_downsample = np.random.choice(i_class1,size = n_class0,replace = False)\n",
    "# 将分类为0的目标向量和下采样的分类为1的目标向量连接起来\n",
    "np.hstack((target[i_class0],target[i_class1_downsample]))\n",
    "# 将分类为0的特征矩阵和下采样的分类为1的特征矩阵连接起来\n",
    "np.vstack((features[i_class0,:],features[i_class1_downsample]))[0:5]\n",
    "\n",
    "# 上采样\n",
    "# 对于每个分类为1的观察值，从分类为0的数据中进行有放回的随机采样\n",
    "i_class0_upsampled = np.random.choice(i_class0,size=n_class1,replace=True)\n",
    "# 将上采样得到的分类为0的目标向量和分类为1的目标向量连接起来\n",
    "np.concatenate((target[i_class0_upsampled],target[i_class1]))\n",
    "# 将上采样得到的分类为0的特征矩阵和分类为1的特征矩阵连接起来\n",
    "np.vstack((features[i_class0_upsampled,:],features[i_class1]))[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################\n",
    "# 将文本编码成词袋（bag of words）\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_data = np.array([\"I love Brazil. Brazil!\",\"Sweden is best\",\"Germany beats both\"])\n",
    "\n",
    "# 创建一个词袋特征矩阵\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "bag_of_words.toarray()\n",
    "\n",
    "count.get_feature_names()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# 处理日期和时间#\n",
    "\n",
    "# 把字符串转换成日期\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "date_strings = np.array(['03-04-2005 11:35 PM',\n",
    "                        '23-05-2010 12:01 AM',\n",
    "                        '04-09-2009 09:09 PM'])\n",
    "\n",
    "# 转换成datetime类型数据\n",
    "[pd.to_datetime(date, format= '%d-%m-%Y %I:%M %p') for date in date_strings]\n",
    "# 添加errors处理错误\n",
    "[pd.to_datetime(date, format= '%d-%m-%Y %I:%M %p',errors=\"coerce\") for date in date_strings]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理时区\n",
    "import pandas as pd\n",
    "\n",
    "pd.Timestamp(\"2017-05-01 06:00:00\",tz='Europe/London')\n",
    "# Timestamp('2017-05-01 06:00:00+0100',tz='Europe/London')\n",
    "\n",
    "date = pd.Timestamp('2017-05-01 06:00:00')\n",
    "date_in_london = date.tz_localize('Europe/London')\n",
    "date_in_london\n",
    "\n",
    "\n",
    "# 改变时区\n",
    "date_in_london.tz_convert(\"Africa/Abidjan\")\n",
    "\n",
    "# 创建3个日期\n",
    "dates = pd.Series(pd.date_range('2/2/2002',periods=3,freq='M'))\n",
    "\n",
    "# 设置时区\n",
    "dates.dt.tz_localize(\"Africa/Abidjan\")\n",
    "\n",
    "from pytz import all_timezones\n",
    "# 查看两个时区\n",
    "all_timezones[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择日期和时间\n",
    "import pandas as pd\n",
    "\n",
    "dataframe = pd.DataFrame()\n",
    "\n",
    "dataframe['date'] = pd.date_range('1/1/2001',periods=100000,freq=\"H\")\n",
    "\n",
    "# 筛选出连个日期之间的观察值\n",
    "dataframe[(dataframe['date'] > '2002-1-1 01:00:00')&(dataframe['date'] <= '2002-1-1 04:00:00')]\n",
    "\n",
    "# 设置索引\n",
    "dataframe = dataframe.set_index(dataframe['date'])\n",
    "# 选择连个日期之间的观测值\n",
    "dataframe.loc['2002-1-1 01:00:00':'2002-1-1 04:00:00']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将日期数据切分成为多个特征\n",
    "import pandas as pd\n",
    "dataframe = pd.DataFrame()\n",
    "dataframe['date']=pd.date_range('1/1/2001',periods=150,freq=\"W\")\n",
    "\n",
    "# 创建年、月、日、时和分的特征\n",
    "dataframe['year'] = dataframe['date'].dt.year\n",
    "dataframe['month'] = dataframe['date'].dt.month\n",
    "dataframe['day'] = dataframe['date'].dt.day\n",
    "dataframe['hour'] = dataframe['date'].dt.hour\n",
    "dataframe['minute'] = dataframe['date'].dt.minute\n",
    "\n",
    "dataframe.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算两个日期之间的时间差\n",
    "import pandas as pd\n",
    "dataframe = pd.DataFrame()\n",
    "\n",
    "dataframe[\"Arrived\"] = [pd.Timestamp('01-01-2017'),pd.Timestamp('01-04-2017')]\n",
    "dataframe['Left'] = [pd.Timestamp('01-01-2017'),pd.Timestamp('01-06-2017')]\n",
    "# 计算两个特征之间的距离\n",
    "dataframe['Left'] - dataframe['Arrived']\n",
    "# 计算两个特征之间的距离（输出结果没有days）\n",
    "pd.Series(delta.days for delta in (dataframe['Left'] - dataframe['Arrived']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对一周内的各天进行编码\n",
    "import pandas as pd\n",
    "# 创建日期\n",
    "dates = pd.Series(pd.date_range('2/2/2002',periods=3,freq='M'))\n",
    "# 查看星期几\n",
    "dates.dt.weekday_name\n",
    "# 查看星期几(仅数值)\n",
    "dates.dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个之后的特征\n",
    "import pandas as pd\n",
    "\n",
    "dataframe = pd.DataFrame()\n",
    "dataframe[\"dates\"] = pd.date_range(\"1/1/2001\",periods=5,freq=\"D\")\n",
    "dataframe[\"stock_price\"] = [1.1,2.2,3.3,4.4,5.5]\n",
    "# 让值滞后一行\n",
    "dataframe[\"previous_days_stock_price\"] = dataframe[\"stock_price\"].shift(1)\n",
    "\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用滚动时间窗口\n",
    "\n",
    "time_index = pd.date_range(\"01/01/2010\",periods=5,freq=\"M\")\n",
    "dataframe = pd.DataFrame(index=time_index)\n",
    "dataframe['Stock_Price'] = [1,2,3,4,5]\n",
    "# 计算滚动平均值\n",
    "dataframe.rolling(window = 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理时间序列中的缺失值\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "time_index = pd.date_range('01/01/2010',periods = 5,freq=\"M\")\n",
    "dataframe = pd.DataFrame(index=time_index)\n",
    "dataframe['Sales'] = [1.0,2.0,np.nan,np.nan,5.0]\n",
    "\n",
    "dataframe.interpolate()\n",
    "\n",
    "# 向前填充\n",
    "dataframe.ffill()\n",
    "\n",
    "# 向后填充\n",
    "dataframe.bfill()\n",
    "\n",
    "# 对缺失值数据进行插值\n",
    "dataframe.interpolate(method='quadratic')\n",
    "\n",
    "# 对缺失值进行插值\n",
    "dataframe.interpolate(limit=1,limit_direction=\"forward\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# 图像处理\n",
    "########################\n",
    "# 加载图像\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 把图像导入成灰度图\n",
    "image = cv2.imread(\"images/plane.jpg\",cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# 显示图像\n",
    "plt.imshow(image ,cmap='gray'),plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "print(type(image))\n",
    "\n",
    "print(image)\n",
    "\n",
    "print(image.shape)\n",
    "\n",
    "print(image[0,0])\n",
    "\n",
    "# 以彩色模式加载图像\n",
    "image_bar = cv2.imread(\"images/plane.jpg\",cv2.IMREAD_COLOR)\n",
    "\n",
    "image_bar[0,0]\n",
    "\n",
    "# OpenCV默认使用BRG格式，而Matplotlib使用RGB格式\n",
    "# 转换为RGB格式\n",
    "image_rgb = cv2.cvtColor(image_bar,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(image_rgb),plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存图像\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 以灰度图的格式导入图像\n",
    "image = cv2.imread(\"images/plane.jpg\",cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "cv2.imwrite('images/plane_new.jpg',image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调整图像大小\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 以灰度图格式导入图像\n",
    "image = cv2.imread(\"images/plane_256x256.jpg\",cv2.IMREAD_GRAYSCALE)\n",
    "# 将图片尺寸调整为50x50像素\n",
    "image_50x50 = cv2.resize(image,(50,50))\n",
    "# 查看图像\n",
    "plt.imshow(image_50x50,cmap='gray'),plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 常见的图像规格有32x32,64x64,96x96,256x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 裁剪图像\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image = cv2.imread(\"images/plane_256x256.jpg\",cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# 选择所有的行和前128列\n",
    "image_cropped = image[:,:128]\n",
    "# 显示图像\n",
    "plt.imshow(image_cropped,cmap='gray'),plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平滑处理图像\n",
    "# 将每个像素的值变换为其相邻像素的平均值\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image = cv2.imread(\"images/plane_256x256.jpg\",cv2.IMREAD_GRAYSCALE)\n",
    "# 平滑处理图像\n",
    "image_blury = cv2.blur(image,(5,5))\n",
    "\n",
    "plt.imshow(image_blury,cmap='gray'),plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 平滑处理图像\n",
    "image_very_blurry = cv2.blur(image,(100,100))\n",
    "\n",
    "plt.imshow(image_very_blurry,cmap='gray'),plt.xticks([]),plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "# 创建核\n",
    "kernel = np.ones((5,5))/25.0\n",
    "\n",
    "# 应用核\n",
    "image_kernel = cv2.filter2D(image,-1,kernel)\n",
    "plt.imshow(image_kernel, cmap='gray'),plt.xticks([]),plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像锐化\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image = cv2.imread('images/plane_256x256.jpg',cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# 创建核\n",
    "kernel = np.array([[0,-1,0],\n",
    "                  [-1,5,-1],\n",
    "                  [0,-1,0]])\n",
    "# 图像锐化\n",
    "image_sharp = cv2.filter2D(image,-1,kernel)\n",
    "plt.imshow(image_sharp,cmap=\"gray\"),plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 提升对比度\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image = cv2.imread(\"images/plane_256x256.jpg\",cv2.IMREAD_GRAYSCALE)\n",
    "image_enhanced = cv2.equalizeHist(image)\n",
    "\n",
    "plt.imshow(image_enhanced,cmap='gray'),plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 彩色图像\n",
    "image_bgr = cv2.imread(\"images/plane.jpg\")\n",
    "# 转换成YUV格式\n",
    "image_yuv = cv2.cvtColor(image_bgr,cv2.COLOR_BGR2YUV)\n",
    "# 对图像应用直方图均衡\n",
    "image_yuv[:,:,0] = cv2.equalizeHist(image_yuv[:,:,0])\n",
    "# 转换成RGB格式\n",
    "image_rgb = cv2.cvtColor(image_yuv,cv2.COLOR_YUV2RGB)\n",
    "\n",
    "plt.imshow(image_rgb),plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 颜色分离\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image_bgr = cv2.imread('images/plane_256x256.jpg')\n",
    "# 将BGR格式转换成HSV格式\n",
    "image_hsv = cv2.cvtColor(image_bgr,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# 定义HSV格式中蓝色分量的区间\n",
    "lower_blue = np.array([50,100,50])\n",
    "upper_blue = np.array([130,255,255])\n",
    "\n",
    "# 创建掩模\n",
    "mask = cv2.inRange(image_hsv,lower_blue,upper_blue)\n",
    "\n",
    "# 应用掩模\n",
    "image_bgr_masked = cv2.bitwise_and(image_bgr,image_bgr,mask=mask)\n",
    "\n",
    "# 从BGR格式转换成RGB格式\n",
    "image_rgb = cv2.cvtColor(image_bgr_masked,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(image_rgb),plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(mask,cmap='gray'),plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 图像二值化(黑白处理)\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image_grey = cv2.imread(\"images/plane_256x256.jpg\",cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# 应用自适应阈值处理\n",
    "max_output_value = 255\n",
    "neighborhood_size = 99\n",
    "subtract_from_mean = 10\n",
    "image_binarized = cv2.adaptiveThreshold(image_grey,max_output_value,cv2.ADAPTIVE_THRESH_GAUSSIAN_C,\n",
    "                                       cv2.THRESH_BINARY,neighborhood_size,subtract_from_mean)\n",
    "\n",
    "plt.imshow(image_binarized,cmap='gray'),plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "image_mean_threshold = cv2.adaptiveThreshold(image_grey,max_output_value,cv2.ADAPTIVE_THRESH_MEAN_C,\n",
    "                                            cv2.THRESH_BINARY,neighborhood_size,subtract_from_mean)\n",
    "plt.imshow(image_mean_threshold,cmap='gray'),plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 移除背景\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 加载图像，并将其转换为RGB格式\n",
    "image_bgr = cv2.imread('images/plane_256x256.jpg')\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# 矩阵的值：左上角的x坐标，左上角的y坐标，宽，高\n",
    "rectangle = (0,56,256,150)\n",
    "\n",
    "# 创建初始掩模\n",
    "mask = np.zeros(image_rgb.shape[:2], np.uint8)\n",
    "\n",
    "# 创建grabCut函数需要的临时数组\n",
    "bgdModel = np.zeros((1,65),np.float64)\n",
    "fgbModel = np.zeros((1,65),np.float64)\n",
    "\n",
    "# 执行grabCut\n",
    "cv2.grabCut(image_bgr,#图像\n",
    "           mask,# 掩模\n",
    "           rectangle,# 矩形\n",
    "           bgdModel,# 背景的临时数组\n",
    "           fgbModel,# 前景的临时数组\n",
    "           5,# 迭代次数\n",
    "           cv2.GC_INIT_WITH_RECT)# 使用定义的矩形初始化\n",
    "\n",
    "# 创建一个掩模，将确定或很可能是背景的部分设置为0，其余部分设置为1\n",
    "mask_2 = np.where((mask==2)|(mask==0),0,1).astype('uint8')\n",
    "\n",
    "# 将图像与掩模相乘以除去背景\n",
    "image_rgb_nobg = image_rgb * mask_2[:,:,np.newaxis]\n",
    "\n",
    "plt.imshow(image_rgb_nobg),plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# 显示掩模\n",
    "plt.imshow(mask, cmap='gray'),plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(mask_2, cmap='gray'),plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 边缘检测\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image_gray = cv2.imread(\"images/plane_256x256.jpg\",cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# 计算像素强度的中位数\n",
    "median_intensity = np.median(image_gray)\n",
    "\n",
    "# 设置阈值\n",
    "lower_threshold = int(max(0,(1.0-0.33)*median_intensity))\n",
    "upper_threshold = int(min(255,(1.0+0.33)*median_intensity))\n",
    "\n",
    "# 应用Canny边缘检测器\n",
    "image_canny = cv2.Canny(image_gray,lower_threshold,upper_threshold)\n",
    "\n",
    "plt.imshow(image_canny,cmap='gray'),plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 角点检测\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image_bgr = cv2.imread('images/plane_256x256.jpg')\n",
    "image_gray = cv2.cvtColor(image_bgr,cv2.COLOR_BGR2GRAY)\n",
    "image_gray = np.float32(image_gray)\n",
    "\n",
    "# 设置角点检测器的参数\n",
    "block_size = 2\n",
    "aperture = 29\n",
    "free_parameter = 0.04\n",
    "\n",
    "# 检测角点\n",
    "detector_responses = cv2.cornerHarris(image_gray,\n",
    "                                    block_size,\n",
    "                                    aperture,\n",
    "                                    free_parameter)\n",
    "# 放大角点标志\n",
    "detector_responses = cv2.dilate(detector_responses,None)\n",
    "\n",
    "# 只保留大于阈值的检测结果，并把它们标记成白色\n",
    "threshold = 0.02\n",
    "image_bgr[detector_responses >\n",
    "         threshold *\n",
    "         detector_responses.max()] = [255,255,255]\n",
    "\n",
    "# 转换成灰度图\n",
    "image_gray = cv2.cvtColor(image_bgr,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "plt.imshow(image_gray,cmap='gray'),plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 显示可能的角点\n",
    "plt.imshow(detector_responses,cmap='gray'),plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "image_bgr = cv2.imread('images/plane_256x256.jpg')\n",
    "image_gray = cv2.cvtColor(image_bgr,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# 待检测角点的数量\n",
    "corners_to_detect = 10\n",
    "minimum_quality_score = 0.05\n",
    "minimum_distance = 25\n",
    "# 检测角点\n",
    "corners = cv2.goodFeaturesToTrack(image_gray,\n",
    "                                 corners_to_detect,\n",
    "                                 minimum_quality_score,minimum_distance)\n",
    "corners = np.float32(corners)\n",
    "# 在每个角点上画白圈\n",
    "for corner in corners:\n",
    "    x, y = corner[0]\n",
    "    cv2.circle(image_bgr,(x,y),10,(255,255,255),-1)\n",
    "\n",
    "# 转换成灰度图\n",
    "image_rgb = cv2.cvtColor(image_bgr,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "plt.imshow(image_rgb,cmap='gray'),plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为机器学习创建特征\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image = cv2.imread(\"images/plane_256x256.jpg\",cv2.IMREAD_GRAYSCALE)\n",
    "# 将图像尺寸转换成10x10\n",
    "image_10x10 = cv2.resize(image,(10,10))\n",
    "# 将图像数据转换成一维向量\n",
    "image_10x10.flatten()\n",
    "\n",
    "plt.imshow(image_10x10,cmap='gray'),plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# 彩色图像\n",
    "image_color = cv2.imread('images/plane_256x256.jpg',cv2.IMREAD_COLOR)\n",
    "image_color_10x10 = cv2.resize(image_color,(10,10))\n",
    "print(image_color_10x10.flatten().shape)\n",
    "\n",
    "# 随着图片尺寸变大，特征的数量也快速增多"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将颜色平均值编码成特征\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "image_bgr = cv2.imread('images/plane_256x256.jpg',cv2.IMREAD_COLOR)\n",
    "#  计算每个通道的平均值\n",
    "channels = cv2.mean(image_bgr)\n",
    "\n",
    "# 交换红色和蓝色通道，将图像从BGR格式转换成RGB格式\n",
    "observation = np.array([(channels[2],channels[1],channels[0])])\n",
    "\n",
    "# 显示每个颜色通道的平均值\n",
    "observation\n",
    "\n",
    "plt.imshow(observation),plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将色彩直方图编码成特征\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "iamge_bgr = cv2.imread(\"images/plane_256x256.jpg\",cv2.IMREAD_COLOR)\n",
    "\n",
    "# 将图像转换成RGB格式\n",
    "image_rgb = cv2.cvtColor(image_bgr,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# 创建一个特征列表\n",
    "features = []\n",
    "\n",
    "# 为每一个通道计算直方图\n",
    "colors = ('r','g','b')\n",
    "# 为每一个通道计算直方图并把它加入特征列表中\n",
    "for i, channel in enumerate(colors):\n",
    "    histogram = cv2.calcHist([image_rgb],# 图像\n",
    "                            [i],# 颜色通道的序号\n",
    "                            None,# 不使用掩模\n",
    "                            [256],# 直方图尺寸\n",
    "                            [0,256])# 范围\n",
    "    features.extend(histogram)\n",
    "\n",
    "# 将样本的特征值展开成一维数组\n",
    "observation = np.array(features).flatten()\n",
    "\n",
    "# 显示样本前5个特征值\n",
    "observation[0:5]\n",
    "\n",
    "image_rgb[0,0]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.Series([1,1,2,2,3,3,3,4,5])\n",
    "\n",
    "data.hist(grid=False)\n",
    "plt.show()\n",
    "\n",
    "# 计算每个颜色通道的直方图\n",
    "colors = ('r','g','b')\n",
    "# 对每个通道绘制直方图\n",
    "for i,channel in enumerate(colors):\n",
    "    histogram = cv2.calcHist([image_rgb],\n",
    "                            [i],\n",
    "                            None,\n",
    "                            [256],\n",
    "                            [0,256])\n",
    "    plt.plot(histogram,color = channel)\n",
    "    plt.xlim([0,256])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# 利用特征提取进行特征降维\n",
    "\n",
    "# 使用主成分进行特征降维\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "# 标准化特征矩阵\n",
    "features = StandardScaler().fit_transform(digits.data)\n",
    "# 创建可以保留99%信息量（用方差表示）的PCA\n",
    "pca = PCA(n_components=0.99,whiten=True)\n",
    "\n",
    "# 执行PCA\n",
    "features_pca = pca.fit_transform(features)\n",
    "\n",
    "# 显示结果\n",
    "print(\"Original number of features:\",features.shape[1])\n",
    "print(\"Reduced number of features:\",features_pca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对线性不可分数据集进行特征降维\n",
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# 创建线性不可分数据\n",
    "features, _ = make_circles(n_samples = 1000,random_state=1,noise=0.1,factor=0.1)\n",
    "\n",
    "# 应用基于径向基函数（Radius Basis Function， RBF）核的Kernel PCA方法\n",
    "kpca = KernelPCA(kernel=\"rbf\",gamma=15,n_components=1)\n",
    "features_kpca = kpca.fit_transform(features)\n",
    "\n",
    "print(\"Original number of features:\",features.shape[1])\n",
    "print(\"Reduced number of features:\",features_kpca.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过最大化类间可分性进行特征降维\n",
    "from sklearn import datasets\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# 创建并运行LDA，然后用他对特征做变换\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "features_lda = lda.fit(features,target).transform(features)\n",
    "\n",
    "# 打印特征的数量\n",
    "print(\"Original number of features:\",features.shape[1])\n",
    "print(\"Reduced number of features:\",features_lda.shape[1])\n",
    "\n",
    "lda.explained_variance_ratio_\n",
    "\n",
    "\n",
    "\n",
    "lda = LinearDiscriminantAnalysis(n_components=None)\n",
    "features_lda = lda.fit(features,target)\n",
    "\n",
    "# 获取方差的百分比数组\n",
    "lda_var_ratios = lda.explained_variance_ratio_\n",
    "# 函数定义\n",
    "def select_n_components(var_ratio,goal_var:float)->int:\n",
    "#     设置总方差的初始值\n",
    "    total_variance = 0.0\n",
    "#     设置特征数量的初始值\n",
    "    n_components = 0\n",
    "#     遍历方差百分比数组的元素\n",
    "    for explained_variance in var_ratio:\n",
    "#         将该百分比加入总方差\n",
    "        total_variance += explained_variance\n",
    "#         n_components的值加1\n",
    "        n_components += 1\n",
    "#         如果达到目标阈值\n",
    "        if total_variance >= goal_var:\n",
    "#             结束遍历\n",
    "            break\n",
    "    \n",
    "#     返回n_components的值\n",
    "    return n_components\n",
    "\n",
    "# 运行函数\n",
    "select_n_components(lda_var_ratios,0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用矩阵分解法进行特征降维\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn import datasets\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "features = digits.data\n",
    "# 创建NMF，进行变换并应用\n",
    "nmf = NMF(n_components=10,random_state=1)\n",
    "features_nmf = nmf.fit_transform(features)\n",
    "\n",
    "\n",
    "print(\"Original number of features:\",features.shape[1])\n",
    "print(\"Reduced number of features:\",features_nmf.shape[1])\n",
    "\n",
    "# 特征矩阵不能包含负值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对稀疏数据进行特征降维\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "# 标准化特征矩阵\n",
    "features = StandardScaler().fit_transform(digits.data)\n",
    "# 生成稀疏矩阵\n",
    "features_sparse = csr_matrix(features)\n",
    "# 创建tsvd\n",
    "tsvd = TruncatedSVD(n_components=10)\n",
    "\n",
    "# 在稀疏矩阵上执行TSVD\n",
    "features_sparse_tsvd = tsvd.fit(features_sparse).transform(features_sparse)\n",
    "\n",
    "\n",
    "print(\"Original number of features:\",features_sparse.shape[1])\n",
    "print(\"Reduced number of features:\",features_sparse_tsvd.shape[1])\n",
    "\n",
    "# 对钱三个成分的信息量占比求和\n",
    "print(tsvd.explained_variance_ratio_[0:3].sum())\n",
    "\n",
    "\n",
    "# 用比原特征数量小1的值作为n_components的值，创建并运行TSVD\n",
    "tsvd = TruncatedSVD(n_components=features_sparse.shape[1]-1)\n",
    "features_tsvd = tsvd.fit(features)\n",
    "\n",
    "# 获取方差百分比数组\n",
    "tsvd_var_ratios = tsvd.explained_variance_ratio_\n",
    "\n",
    "# 函数定义\n",
    "def select_n_components(var_ratio,goal_var):\n",
    "#     设置总方差的初始值\n",
    "    total_variance = 0.0\n",
    "    \n",
    "#     设置特征数量的初始值\n",
    "    n_components = 0\n",
    "#     遍历方差百分比数组的元素\n",
    "    for explained_variance in var_ratio:\n",
    "#         将该百分比加入总方差\n",
    "        total_variance += explained_variance\n",
    "#         n_components的值加1\n",
    "        n_components += 1\n",
    "        if total_variance >= goal_var:\n",
    "#             结束遍历\n",
    "            break;\n",
    "#     返回n_components的值\n",
    "    return n_components\n",
    "\n",
    "select_n_components(tsvd_var_ratios,0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# 使用特征选择进行降维\n",
    "\n",
    "# 数值型特征方差的阈值化\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# 创建VarianceThreshold对象\n",
    "thresholder = VarianceThreshold(threshold=.5)\n",
    "# 创建大方差特征矩阵\n",
    "features_high_variance = thresholder.fit_transform(features)\n",
    "# 显示大方差特征矩阵\n",
    "features_high_variance[0:3]\n",
    "\n",
    "# 条件\n",
    "# 方差不是中心化的\n",
    "# 方差的阈值是手动选择的\n",
    "\n",
    "# 显示方差\n",
    "thresholder.fit(features).variances_\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "selector = VarianceThreshold()\n",
    "selector.fit(features_std).variances_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 二值特征的方差阈值化\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# 特征0:80%为分类0\n",
    "# 特征1:80%为分类1\n",
    "# 特征2:60%为分类0,40%为分类1\n",
    "features = [[0,1,0],\n",
    "           [0,1,1],\n",
    "           [0,1,0],\n",
    "           [0,1,1],\n",
    "           [1,0,0]]\n",
    "\n",
    "# 创建VarianceThreshold对象并运行\n",
    "thresholder = VarianceThreshold(threshold=(.75*(1-.75)))\n",
    "thresholder.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理高度相关性的特征\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "features = np.array([[1,1,1],\n",
    "                    [2,2,0],\n",
    "                    [3,3,1],\n",
    "                    [4,4,0],\n",
    "                    [5,5,1],\n",
    "                    [6,6,0],\n",
    "                    [7,7,1],\n",
    "                    [8,7,0],\n",
    "                    [9,7,1]])\n",
    "\n",
    "dataframe = pd.DataFrame(features)\n",
    "\n",
    "# 创建相关矩阵\n",
    "corr_matrix = dataframe.corr().abs()\n",
    "# 选择相关矩阵的上三角阵\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape),k=1).astype(np.bool))\n",
    "\n",
    "# 找到相关性大于0.95的特征列的索引\n",
    "to_drop = [column for column in upper.columns if any(upper[column] >0.95)]\n",
    "# 删除特征\n",
    "dataframe.drop(dataframe.columns[to_drop],axis=1).head(3)\n",
    "\n",
    "# 相关矩阵\n",
    "dataframe.corr()\n",
    "# 相关矩阵的上三角矩阵\n",
    "upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 删除与分类任务不相关的特征\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2,f_classif\n",
    "\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# 将分类数据转换成整数数据\n",
    "features =  features.astype(int)\n",
    "\n",
    "# 选择卡方统计量最大的两个特征\n",
    "chi2_selector = SelectKBest(chi2,k=2)\n",
    "features_kbest = chi2_selector.fit_transform(features,target)\n",
    "\n",
    "print(\"Original number of features:\",features.shape[1])\n",
    "print(\"Reduced number of features:\",features_kbest.shape[1])\n",
    "\n",
    "# 选择F值最大的两个特征\n",
    "fvalue_selector = SelectKBest(f_classif,k=2)\n",
    "features_kbest = fvalue_selector.fit_transform(features,target)\n",
    "\n",
    "print(\"Original number of features:\",features.shape[1])\n",
    "print(\"Reduced number of features:\",features_kbest.shape[1])\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "\n",
    "# 选择F值位于前75%的特征\n",
    "fvalue_selector = SelectPercentile(f_classif,percentile=75)\n",
    "features_kbest = fvalue_selector.fit_transform(features,target)\n",
    "\n",
    "print(\"Original number of features:\",features.shape[1])\n",
    "print(\"Reduced number of features:\",features_kbest.shape[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 递归式特征消除\n",
    "import warnings\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn import datasets, linear_model\n",
    "\n",
    "# 忽略一些烦人但无害的警告信息\n",
    "warnings.filterwarnings(action='ignore',module='scipy',message='^internal gelsd')\n",
    "\n",
    "# 生成特征矩阵、目标向量以及系数\n",
    "features,target = make_regression(n_samples=10000,n_features=100,n_informative = 2,random_state =1)\n",
    "\n",
    "# 创建线性回归对象\n",
    "ols = linear_model.LinearRegression()\n",
    "\n",
    "# 递归消除特征\n",
    "rfecv = RFECV(estimator=ols,step=1,scoring=\"neg_mean_squared_error\")\n",
    "rfecv.fit(features,target)\n",
    "rfecv.transform(features)\n",
    "\n",
    "# 最优特征的数量\n",
    "rfecv.n_features_\n",
    "\n",
    "# 哪些特征是最优特征\n",
    "rfecv.support_\n",
    "\n",
    "# 将特征从最好（1）到最差排序\n",
    "rfecv.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# 模型评估\n",
    "# 交叉验证模型\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "digits = datasets.load_digits()\n",
    "\n",
    "features = digits.data\n",
    "\n",
    "target = digits.target\n",
    "\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "logit = LogisticRegression()\n",
    "# 创建包含数据标准化和逻辑回归的流水线\n",
    "pipeline = make_pipeline(standardizer,logit)\n",
    "\n",
    "# 创建k折交叉验证对象\n",
    "kf = KFold(n_splits=10,shuffle=True ,random_state=1)\n",
    "\n",
    "# 执行k折交叉验证\n",
    "cv_result = cross_val_score(pipeline,# 流水线\n",
    "                           features,# 特征矩阵\n",
    "                           target,# 目标向量\n",
    "                           cv=kf,# 交叉验证方法\n",
    "                           scoring='accuracy',# 损失函数\n",
    "                           n_jobs=-1)# 使用所有的CPU核\n",
    "\n",
    "# 计算得分的平均值\n",
    "cv_result.mean()\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "features_train,features_test,target_train,target_test = train_test_split(features,target,test_size=0.1,random_state=1)\n",
    "# 使用训练集计算标准化参数\n",
    "standardizer.fit(features_train)\n",
    "# 将标准化操作应用到训练集和测试集\n",
    "features_train_std = standardizer.transform(features_train)\n",
    "features_test_std = standardizer.transform(features_test)\n",
    "\n",
    "# 创建一个流水线\n",
    "pipeline = make_pipeline(standardizer,logit)\n",
    "# 执行k折交叉验证\n",
    "cv_result = cross_val_score(pipeline,# 流水线\n",
    "                           features,# 特征矩阵\n",
    "                           target,# 目标向量\n",
    "                           cv=kf,# 交叉验证方法\n",
    "                           scoring='accuracy',# 损失函数\n",
    "                           n_jobs=-1)# 使用所有的CPU核\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个基准回归模型\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "boston = load_boston()\n",
    "\n",
    "features,target = boston.data,boston.target\n",
    "\n",
    "features_train, features_test, target_train,target_test = train_test_split(features,target,random_state = 0)\n",
    "\n",
    "# 创建DummyRegressor对象\n",
    "dummy = DummyRegressor(strategy='mean')\n",
    "# 训练回归模型\n",
    "dummy.fit(features_train,target_train)\n",
    "# R方得分\n",
    "dummy.score(features_test,target_test)\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# 训练简单的线性回归模型\n",
    "ols = LinearRegression()\n",
    "ols.fit(features_train,target_train)\n",
    "ols.score(features_test,target_test)\n",
    "\n",
    "# 创建一个将所有样本预测为20的DummyRegressor\n",
    "clf = DummyRegressor(strategy='constant',constant=20)\n",
    "clf.fit(features_train,target_train)\n",
    "clf.score(features_test,target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个基准分类模型\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "iris = load_iris()\n",
    "features, target = iris.data,iris.target\n",
    "features_train,features_test,target_train,target_test = train_test_split(\n",
    "    features,target,random_state=0)\n",
    "\n",
    "dummy = DummyClassifier(strategy='uniform',random_state=1)\n",
    "\n",
    "dummy.fit(features_train,target_train)\n",
    "\n",
    "dummy.score(features_test,target_test)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier()\n",
    "classifier.fit(features_train,target_train)\n",
    "\n",
    "classifier.score(features_test,target_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X,y = make_classification(n_samples = 10000,\n",
    "                         n_features=3,\n",
    "                         n_informative=3,\n",
    "                         n_redundant=0,\n",
    "                         n_classes=2,\n",
    "                         random_state=1)\n",
    "logit = LogisticRegression()\n",
    "# 准确率\n",
    "cross_val_score(logit,X,y,scoring='accuracy')\n",
    "# 精确度\n",
    "cross_val_score(logit,X,y,scoring='precision')\n",
    "# 召回率\n",
    "cross_val_score(logit,X,y,scoring='recall')\n",
    "# F1分数\n",
    "cross_val_score(logit,X,y,scoring='f1')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1,random_state=1)\n",
    "y_hat = logit.fit(X_train,y_train).predict(X_test)\n",
    "accuracy_score(y_test,y_hat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估 二元分类器的阈值\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve,roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features, target = make_classification(n_samples=10000,\n",
    "                                      n_features=10,\n",
    "                                      n_classes=2,\n",
    "                                      n_informative=3,\n",
    "                                      random_state=3)\n",
    "\n",
    "features_train,features_test,target_train,target_test=train_test_split(\n",
    "                features,target,test_size=0.1,random_state=1)\n",
    "logit = LogisticRegression()\n",
    "\n",
    "logit.fit(features_train,target_train)\n",
    "\n",
    "# 获取预测的概率\n",
    "target_probabilities = logit.predict_proba(features_test)[:,1]\n",
    "\n",
    "# 计算真阳性和假阳性的概率\n",
    "false_positive_rate, true_positive_rate,threshold = roc_curve(target_test,target_probabilities)\n",
    "\n",
    "# 画出ROC曲线\n",
    "plt.title(\"Receiver Operating Characteristic\")\n",
    "plt.plot(false_positive_rate,true_positive_rate)\n",
    "plt.plot([0,1],ls=\"--\")\n",
    "plt.plot([0,0],[1,0],c=\".7\"),plt.plot([1,1],c=\".7\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.show()\n",
    "\n",
    "# 获取预测概率\n",
    "logit.predict_proba(features_test)[0:1]\n",
    "logit.classes_\n",
    "\n",
    "\n",
    "print(\"Threshold:\",threshold[116])\n",
    "print(\"True Positive Rate:\",true_positive_rate[116])\n",
    "print(\"False Positive Rate:\",false_positive_rate[116])\n",
    "\n",
    "print(\"Threshold:\",threshold[45])\n",
    "print(\"True Positive Rate:\",true_positive_rate[45])\n",
    "print(\"False Positive Rate:\",false_positive_rate[45])\n",
    "\n",
    "# 计算ROC曲线下方的面积\n",
    "roc_auc_score(target_test,target_probabilities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估多元分类器\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "features, target = make_classification(n_samples=10000,\n",
    "                                      n_features=3,\n",
    "                                      n_informative=3,\n",
    "                                      n_redundant=0,\n",
    "                                      n_classes=3,\n",
    "                                      random_state=1)\n",
    "\n",
    "logit = LogisticRegression()\n",
    "cross_val_score(logit,features,target,scoring='accuracy')\n",
    "\n",
    "# 使用macro-F1分数对模型进行交叉验证\n",
    "cross_val_score(logit,features,target,scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分类器性能的可视化\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# 创建目标分类的名称列表\n",
    "class_name = iris.target_names\n",
    "\n",
    "features_train,features_test,target_train,target_test = train_test_split(\n",
    "        features,target,random_state=1)\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "target_predicted = classifier.fit(features_train,target_train).predict(features_test)\n",
    "\n",
    "matrix = confusion_matrix(target_test,target_predicted)\n",
    "dataframe = pd.DataFrame(matrix,index=class_name,columns=class_name)\n",
    "\n",
    "# 绘制热力图\n",
    "sns.heatmap(dataframe,annot=True,cbar=None,cmap='Blues')\n",
    "plt.title(\"Confusion Matrix\"),plt.tight_layout()\n",
    "plt.ylabel(\"True Class\"),plt.xlabel('Predicted Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估回归模型\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# 生成特征矩阵和目标向量\n",
    "features, target = make_regression(n_samples=100,\n",
    "                                   n_features=3,\n",
    "                                   n_informative=3,\n",
    "                                   noise=50,\n",
    "                                  coef=False,\n",
    "                                  random_state=1)\n",
    "\n",
    "ols = LinearRegression()\n",
    "\n",
    "# 使用MSE对线性回归做交叉验证\n",
    "cross_val_score(ols,features,target,scoring='neg_mean_squared_error')\n",
    "\n",
    "# 使用决定系数对线性回归进行交叉验证\n",
    "cross_val_score(ols,features,target,scoring='r2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估聚类模型\n",
    "import numpy as np\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn import datasets\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "features,_ = make_blobs(n_samples=1000,\n",
    "                       n_features=10,\n",
    "                       centers=2,\n",
    "                       cluster_std=0.5,\n",
    "                       shuffle=True,\n",
    "                       random_state=1)\n",
    "\n",
    "model = KMeans(n_clusters=2,random_state=1).fit(features)\n",
    "target_predicted = model.labels_\n",
    "# 模型评估\n",
    "silhouette_score(features,target_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建自定义评估指标\n",
    "from sklearn.metrics import make_scorer,r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import make_regression\n",
    "\n",
    "features,target = make_regression(n_samples=100,\n",
    "                                 n_features=3,\n",
    "                                 random_state=1)\n",
    "features_train,features_test,target_train,target_test=train_test_split(\n",
    "                features,target,test_size=0.10,random_state=1)\n",
    "\n",
    "# 创建自定义指标函数\n",
    "def custom_metric(target_test,target_predicted):\n",
    "#     计算R方得分\n",
    "    r2 = r2_score(target_test,target_predicted)\n",
    "#     返回R方得分\n",
    "    return r2\n",
    "\n",
    "# 创建评分函数（评分器），并定义分数越高代表模型越好\n",
    "score = make_scorer(custom_metric,greater_is_better=True)\n",
    "# 创建岭回归（Ridge Regression）对象\n",
    "classifier = Ridge()\n",
    "# 训练岭回归模型\n",
    "model = classifier.fit(features_train,target_train)\n",
    "score(model,features_test,target_test)\n",
    "\n",
    "target_predicted = model.predict(features_test)\n",
    "r2_score(target_test,target_predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#可视化训练集规模的影响\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "digits = load_digits()\n",
    "features,target=digits.data,digits.target\n",
    "\n",
    "train_sizes,train_scores,test_scores = learning_curve(# 分类器\n",
    "                                                      RandomForestClassifier(),\n",
    "                                                      # 特征矩阵\n",
    "                                                      features,\n",
    "                                                      # 目标向量\n",
    "                                                      target,\n",
    "                                                      # 交叉验证的折数\n",
    "                                                      cv=10,\n",
    "                                                      # 性能指标\n",
    "                                                      scoring='accuracy',\n",
    "                                                      # 使用所有CPU核\n",
    "                                                      n_jobs=-1,\n",
    "                                                      # 50个训练集的规模\n",
    "                                                      train_sizes=np.linspace(0.01,1.0,50))\n",
    "# 计算训练得分的平均值和标准差\n",
    "train_mean = np.mean(train_scores,axis=1)\n",
    "train_std = np.std(train_scores,axis=1)\n",
    "# 计算测试集得分的平均值和标准差\n",
    "test_mean = np.mean(test_scores,axis=1)\n",
    "test_std = np.std(test_scores,axis=1)\n",
    "\n",
    "# 画线\n",
    "plt.plot(train_sizes,train_mean,'--',color='#111111',label='Training score')\n",
    "plt.plot(train_sizes,test_mean,color='#111111',label='Cross-validation score')\n",
    "# 画带状线\n",
    "plt.fill_between(train_sizes,train_mean-train_std,\n",
    "                train_mean+train_std,color='#DDDDDD')\n",
    "plt.fill_between(train_sizes,test_mean-test_std,\n",
    "                test_mean+test_std,color='#DDDDDD')\n",
    "# 创建图\n",
    "plt.title(\"Learning Curve\")\n",
    "plt.xlabel('Training Set Size'),plt.ylabel('Accuracy Score')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "# 创建目标分类名的列表\n",
    "class_names = iris.target_names\n",
    "features_train,features_test,target_train,target_test = train_test_split(\n",
    "            features,target,random_state=1)\n",
    "classifier = LogisticRegression()\n",
    "\n",
    "model = classifier.fit(features_train,target_train)\n",
    "target_predicted = model.predict(features_test)\n",
    "\n",
    "# 生成分类器的性能报告\n",
    "print(classification_report(target_test,target_predicted,target_names=class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化超参数值的效果\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "digits = load_digits()\n",
    "features,target=digits.data,digits.target\n",
    "\n",
    "# 创建参数的变化范围\n",
    "param_range = np.arange(1,250,2)\n",
    "\n",
    "# 对区间内的参数值分别计算模型在训练集和测试集上的准确率\n",
    "train_scores,test_scores = validation_curve(\n",
    "    # 分类器\n",
    "    RandomForestClassifier(),\n",
    "    # 特征矩阵\n",
    "    features,\n",
    "    # 目标向量\n",
    "    target,\n",
    "    # 要查看的超参数\n",
    "    param_name = 'n_estimators',\n",
    "    # 超参数值的范围\n",
    "    param_range=param_range,\n",
    "    # 交叉验证的折数\n",
    "    cv=3,\n",
    "    # 性能指标\n",
    "    scoring='accuracy',\n",
    "    # 使用所有CPU核\n",
    "    n_jobs=-1)\n",
    "\n",
    "train_mean = np.mean(train_scores,axis=1)\n",
    "train_std = np.std(train_scores,axis=1)\n",
    "\n",
    "test_mean = np.mean(test_scores,axis=1)\n",
    "test_std = np.std(test_scores,axis=1)\n",
    "\n",
    "\n",
    "plt.plot(param_range,train_mean,label='Training score',color='black')\n",
    "plt.plot(param_range,test_mean,label='Cross-validation score',color='dimgrey')\n",
    "\n",
    "plt.fill_between(param_range,train_mean-train_std,\n",
    "                 train_mean+train_std,color='gray')\n",
    "plt.fill_between(param_range,test_mean-test_std,\n",
    "                test_mean+test_std,color='gainsboro')\n",
    "\n",
    "plt.title(\"Validation Curve With Random Forest\")\n",
    "plt.xlabel('Number Of Trees'),plt.ylabel('Accuracy Score')\n",
    "plt.legend(loc='best')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################\n",
    "# 模型选择\n",
    "##############################\n",
    "\n",
    "# 使用穷举搜索选择最佳模型\n",
    "import numpy as np\n",
    "from sklearn import linear_model,datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "# 创建正则化惩罚的候选超参数区域\n",
    "penalty = ['l1','l2']\n",
    "\n",
    "# 创建正则化候选超参数区域\n",
    "C = np.logspace(0,4,10)\n",
    "\n",
    "# 创建候选超参数的字典\n",
    "hyperarmeters = dict(C=C,penalty=penalty)\n",
    "\n",
    "# 创建网格搜索对象\n",
    "gridsearch = GridSearchCV(logistic, hyperarmeters,cv=5,verbose=0)\n",
    "\n",
    "# 训练网格搜索\n",
    "best_model = gridsearch.fit(features,target)\n",
    "\n",
    "np.logspace(0,4,10)\n",
    "\n",
    "# 查看最佳超参数\n",
    "print('Best Penalty:',best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:',best_model.best_estimator_.get_params()['C'])\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)\n",
    "\n",
    "best_model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用随机搜索选择最佳模型\n",
    "from scipy.stats import uniform\n",
    "from sklearn import linear_model,datasets\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "penalty = ['l1','l2']\n",
    "\n",
    "C = uniform(loc=0,scale=4)\n",
    "\n",
    "hyperarmeters = dict(C=C, penalty=penalty)\n",
    "\n",
    "# 创建随机搜索对象\n",
    "randomizedsearch = RandomizedSearchCV(\n",
    "        logistic,hyperarmeters,\n",
    "        random_state=1,\n",
    "        n_iter=100,\n",
    "        cv=5,verbose=0,\n",
    "        n_jobs=-1)\n",
    "\n",
    "# 训练随机搜索\n",
    "best_model = randomizedsearch.fit(features,target)\n",
    "\n",
    "# 定义区间（0,4）上的均匀分布，并从中抽取10个样本值\n",
    "uniform(loc=0,scale=4).rvs(10)\n",
    "\n",
    "# 查看最佳超参数\n",
    "print('Best Penalty:',best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:',best_model.best_estimator_.get_params()['C'])\n",
    "\n",
    "best_model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从多种学习算法中选择最佳模型\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# 创建流水线\n",
    "pipe =  Pipeline([('classifier',RandomForestClassifier())])\n",
    "\n",
    "# 创建候选学习算法及超参数的字典\n",
    "search_space = [{'classifier':[LogisticRegression()],\n",
    "                'classifier__penalty':['l1','l2'],\n",
    "                'classifier__C':np.logspace(0,4,10)},\n",
    "               {'classifier':[RandomForestClassifier()],\n",
    "               'classifier__n_estimators':[10,100,1000],\n",
    "               'classifier__max_features':[1,2,3]}]\n",
    "\n",
    "# 创建GridSearchCV对象\n",
    "gridsearch = GridSearchCV(pipe,search_space,cv=5,verbose=0)\n",
    "\n",
    "# 执行网格搜索\n",
    "best_model = gridsearch.fit(features,target)\n",
    "\n",
    "best_model.best_estimator_.get_params()['classifier']\n",
    "best_model.predict(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 将数据预处理加入模型选择过程\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# 创建一个包含StandardScaler和PCA的预处理对象\n",
    "preprocess = FeatureUnion([(\"std\",StandardScaler()),(\"pca\",PCA())])\n",
    "\n",
    "# 创建一个流水线\n",
    "pipe = Pipeline([(\"preprocess\",preprocess),\n",
    "                (\"classifier\",LogisticRegression())])\n",
    "\n",
    "# 创建候选值的取值空间\n",
    "search_space = [{\"preprocess__pca__n_components\":[1,2,3],\n",
    "                \"classifier__penalty\": [\"l1\",\"l2\"],\n",
    "                 \"classifier__C\": np.logspace(0,4,10)}]\n",
    "\n",
    "# 创建网格搜索对象\n",
    "clf = GridSearchCV(pipe,search_space,cv=5,verbose=0,n_jobs=-1)\n",
    "\n",
    "best_model = clf.fit(features ,target)\n",
    "\n",
    "best_model.best_estimator_.get_params()['preprocess__pca__n_components']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用并行化加速模型选择\n",
    "import numpy as np\n",
    "from sklearn import linear_model,datasets\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "penalty = ['l1','l2']\n",
    "C = np.logspace(0,4,1000)\n",
    "hyperparameters = dict(C=C,penalty=penalty)\n",
    "\n",
    "gridsearch = GridSearchCV(logistic,hyperparameters,cv=5,n_jobs=-1,verbose=1)\n",
    "\n",
    "best_model = gridsearch.fit(features,target)\n",
    "\n",
    "# 创建使用点那个核的网格搜索\n",
    "clf = GridSearchCV(logistic,hyperparameters,cv=5,n_jobs=1,verbose=1)\n",
    "# 执行网格搜索\n",
    "best_model = clf.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用针对特定算法的方法加速模型选择\n",
    "from sklearn import linear_model,datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = target\n",
    "\n",
    "logit = linear_model.LogisticRegressionCV(Cs=100)\n",
    "\n",
    "logit.fit(features,target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型选择后的性能评估\n",
    "import numpy as np\n",
    "from sklearn import linear_model,datasets\n",
    "from sklearn.model_selection import GridSearchCV,cross_val_score\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "logistic = linear_model.LogisticRegression()\n",
    "\n",
    "C = np.logspace(0,4,20)\n",
    "\n",
    "hyperparameters = dict(C=C)\n",
    "\n",
    "gridsearch = GridSearchCV(logistic,hyperparameters,cv=5,n_jobs=-1,verbose=0)\n",
    "\n",
    "cross_val_score(gridsearch,features,target).mean()\n",
    "\n",
    "\n",
    "gridsearch = GridSearchCV(logistic,hyperparameters,cv=5,verbose=1)\n",
    "best_model = gridsearch.fit(features,target)\n",
    "\n",
    "scores = cross_val_score(gridsearch,features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# 线性回归\n",
    "#######################################\n",
    "\n",
    "# 拟合一条直线\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "features = boston.data[:,0:2]\n",
    "target = boston.target\n",
    "\n",
    "regression = LinearRegression()\n",
    "model = regression.fit(features,target)\n",
    "\n",
    "# 查看截距\n",
    "model.intercept_\n",
    "# 显示权重\n",
    "model.coef_\n",
    "\n",
    "target[0]*1000\n",
    "\n",
    "# 预测\n",
    "model.predict(features)[0]*10000\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理特征之间的影响\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "boston = load_boston()\n",
    "features = boston.data[:,0:2]\n",
    "target = boston.target\n",
    "\n",
    "# 创建交互特征\n",
    "interaction = PolynomialFeatures(\n",
    "    degree=3,include_bias=False,interaction_only=True)\n",
    "features_interaction = interaction.fit_transform(features)\n",
    "\n",
    "regression = LinearRegression()\n",
    "\n",
    "model = regression.fit(features_interaction,target)\n",
    "\n",
    "features[0]\n",
    "\n",
    "import numpy as np\n",
    "# 将每个样本的第一个和第二个特征相乘\n",
    "interaction_term = np.multiply(features[:,0],features[:,1])\n",
    "\n",
    "# 查看第一个样本的交互特征\n",
    "interaction_term[0]\n",
    "\n",
    "# 观察第一个样本的值\n",
    "features_interaction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 拟合非线性关系\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "boston = load_boston()\n",
    "features = boston.data[:,0:1]\n",
    "target = boston.target\n",
    "\n",
    "# 创建多项式特征x^2和x^3\n",
    "polynomial = PolynomialFeatures(degree=3,include_bias=False)\n",
    "features_polynomial = polynomial.fit_transform(features)\n",
    "\n",
    "regression = LinearRegression()\n",
    "model = regression.fit(features_polynomial,target)\n",
    "\n",
    "features_polynomial[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过正则化减少方差\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "boston = load_boston()\n",
    "features = boston.data\n",
    "target = boston.target\n",
    "\n",
    "# 特征标准化\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "# 创建一个包含指定alpha值的岭回归\n",
    "regression = Ridge(alpha=0.5)\n",
    "model = regression.fit(features_standardized,target)\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "# 创建包含三个alpha值的RidgeCV对象\n",
    "regr_cv = RidgeCV(alphas=[0.1,1.0,10.0])\n",
    "\n",
    "model_cv = regr_cv.fit(features_standardized,target)\n",
    "\n",
    "# 系数\n",
    "model_cv.coef_\n",
    "\n",
    "# alpha值\n",
    "model_cv.alpha_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用套索回归减少特征\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "boston = load_boston()\n",
    "features = boston.data\n",
    "target = boston.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "# 创建套索回归并指定alpha值\n",
    "regression = Lasso(alpha=0.5)\n",
    "model =regression.fit(features_standardized,target)\n",
    "\n",
    "model.coef_\n",
    "\n",
    "# 创建一个alpha值为10的套索回归\n",
    "regression_a10=Lasso(alpha=10)\n",
    "model_a10=regression_a10.fit(features_standardized,target)\n",
    "\n",
    "model_a10.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# 树和森林\n",
    "#############################\n",
    "\n",
    "# 训练决策树分类器\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "decisiontree = DecisionTreeClassifier(random_state=0)\n",
    "model = decisiontree.fit(features,target)\n",
    "\n",
    "# 创建新样本\n",
    "observation = [[5,4,3,2]]\n",
    "# 预测样本的分类\n",
    "model.predict(observation)\n",
    "# 查看样本分别属于三个分类的概率\n",
    "model.predict_proba(observation)\n",
    "# 使用entropy作为不纯度检测方法创建决策树分类器对象\n",
    "decisiontree_entropy = DecisionTreeClassifier(\n",
    "    criterion='entropy',random_state=0)\n",
    "\n",
    "model_entropy = decisiontree_entropy.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练决策树回归模型\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn import datasets\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "features = boston.data[:,0:2]\n",
    "target = boston.target\n",
    "\n",
    "decisiontree = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "model = decisiontree.fit(features,target)\n",
    "\n",
    "# 创建新样本\n",
    "observation = [[0.02,16]]\n",
    "# 预测样本值\n",
    "model.predict(observation)\n",
    "\n",
    "# 使用MAE创建决策树回归模型\n",
    "decisiontree_mae = DecisionTreeRegressor(criterion='mae',random_state=0)\n",
    "model_mae = decisiontree_mae.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化决策树模型\n",
    "import pydotplus\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "from IPython.display import Image\n",
    "from sklearn import tree\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "decisiontree = DecisionTreeClassifier(random_state=0)\n",
    "model = decisiontree.fit(features,target)\n",
    "\n",
    "dot_data = tree.export_graphviz(decisiontree,\n",
    "                               out_file=None,\n",
    "                                feature_names=iris.feature_names,\n",
    "                                   class_names=iris.target_names)\n",
    "\n",
    "# 绘制图形\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "\n",
    "# #由于装了MATLAB以下代码运行不了\n",
    "# 显示图形\n",
    "# Image(graph.create_())\n",
    "\n",
    "# 创建PDF\n",
    "# graph.write_pdf(\"iris.pdf\")\n",
    "\n",
    "# 创建PNG\n",
    "# graph.write_png(\"iris.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练随机森林分类器\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "randomforest = RandomForestClassifier(random_state=0,n_jobs=-1)\n",
    "model = randomforest.fit(features,target)\n",
    "\n",
    "observation = [[5,4,3,2]]\n",
    "model.predict(observation)\n",
    "\n",
    "\n",
    "# 使用熵创建随机森林分类器对象\n",
    "randomforest_entropy = RandomForestClassifier(\n",
    "    criterion='entropy',random_state=0)\n",
    "\n",
    "model_entropy = randomforest_entropy.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练随机森林回归模型\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import datasets\n",
    "\n",
    "boston = datasets.load_boston()\n",
    "features = boston.data[:,0:2]\n",
    "target = boston.target\n",
    "\n",
    "randomforest = RandomForestRegressor(random_state=0,n_jobs=-1)\n",
    "model = randomforest.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 识别随机森林中的重要特征\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target \n",
    "\n",
    "randomforest = RandomForestClassifier(random_state=0,n_jobs=-1)\n",
    "\n",
    "model = randomforest.fit(features,target)\n",
    "\n",
    "# 计算特征的重要性\n",
    "importances = model.feature_importances_\n",
    "\n",
    "# 将特征的重要性按降序排列\n",
    "indices = np.argsort(importances)[::-1]\n",
    "# 按照特征的中要求对特征名称重新排序\n",
    "names = [iris.feature_names[i] for i in indices]\n",
    "\n",
    "# 创建图\n",
    "plt.figure()\n",
    "plt.title('Feature Importance')\n",
    "plt.bar(range(features.shape[1]),importances[indices])\n",
    "plt.xticks(range(features.shape[1]),names,rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 选择随机森林中的重要特征\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "randomforest = RandomForestClassifier(random_state=0,n_jobs=-1)\n",
    "selector = SelectFromModel(randomforest,threshold=0.3)\n",
    "features_important = selector.fit_transform(features,target)\n",
    "model = randomforest.fit(features_important,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理不均衡的分类\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "# 删除前40个样本以获得高纯度不均衡的数据\n",
    "features = features[40:,:]\n",
    "target = target[40:]\n",
    "\n",
    "# 创建目标向量表明分类为0还是1\n",
    "target = np.where((target == 0),0,1)\n",
    "\n",
    "randomforest = RandomForestClassifier(\n",
    "    random_state=0,n_jobs=-1,class_weight='balanced')\n",
    "\n",
    "model = randomforest.fit(features,target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 控制决策树的规模\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "decisiontree = DecisionTreeClassifier(random_state=0,\n",
    "                                     max_depth=None,\n",
    "                                     min_impurity_split=2,\n",
    "                                     min_samples_leaf=1,\n",
    "                                     min_weight_fraction_leaf=0,\n",
    "                                     max_leaf_nodes=None,\n",
    "                                     min_impurity_decrease=0)\n",
    "\n",
    "model = decisiontree.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过boosting提高性能\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "adaboost = AdaBoostClassifier(random_state=0)\n",
    "model = adaboost.fit(features,target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用袋外误差（Out-of-Bag Error）评估随机森林模型\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "randomforest = RandomForestClassifier(\n",
    "    random_state=0,n_estimators=1000,oob_score=True,n_jobs=-1)\n",
    "\n",
    "model = randomforest.fit(features,target)\n",
    "\n",
    "# 查看袋外误差\n",
    "randomforest.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "# KNN\n",
    "#################################\n",
    "\n",
    "# 找到一个观察值的最邻近\n",
    "from sklearn import datasets\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "features_standardized = standardizer.fit_transform(features)\n",
    "\n",
    "# 两个最近的观察值\n",
    "nearest_neighbors = NearestNeighbors(n_neighbors=2).fit(features_standardized)\n",
    "# 创建一个特征值\n",
    "new_observation = [1,1,1,1]\n",
    "# 获取离观察值最近的两个观察值的索引，以及到这两个点的距离\n",
    "distances, indices = nearest_neighbors.kneighbors([new_observation])\n",
    "# 查看最近的两个观察值\n",
    "features_standardized[indices]\n",
    "\n",
    "# 找到按照欧式距离来算最近的两个邻居\n",
    "nearestneighbors_euclidean = NearestNeighbors(\n",
    "    n_neighbors=2,metric='euclidean').fit(features_standardized)\n",
    "\n",
    "# 查看distances\n",
    "distances\n",
    "\n",
    "# 寻找每个观察值按照欧式距离计算的最近的3个邻居（包括它自己）\n",
    "nearestneighbors_euclidean = NearestNeighbors(\n",
    "    n_neighbors=3,metric='euclidean').fit(features_standardized)\n",
    "\n",
    "# 每个观察值和它最近的3个邻居列表（包括它自己）\n",
    "nearest_neighbors_with_self = nearestneighbors_euclidean.kneighbors_graph(\n",
    "    features_standardized).toarray()\n",
    "\n",
    "# 从最近邻居的列表里移除自己\n",
    "for i,x in enumerate(nearest_neighbors_with_self):\n",
    "    x[i]=0\n",
    "    \n",
    "# 查看离第一个观察值最近的两个邻居\n",
    "nearest_neighbors_with_self[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个KNN分类器\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "X_std = standardizer.fit_transform(X)\n",
    "\n",
    "# 训练一个有5个邻居的KNN分类器\n",
    "knn = KNeighborsClassifier(n_neighbors=5,n_jobs=-1).fit(X_std,y)\n",
    "\n",
    "new_observation = [[0.75,0.75,0.75,0.75],[1,1,1,1]]\n",
    "knn.predict(new_observation)\n",
    "\n",
    "\n",
    "# 查看每个观察值分别属于3个分类中的某一个的概率\n",
    "knn.predict_proba(new_observation)\n",
    "\n",
    "knn.predict(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 确定最佳的领域点集的大小\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "standardizer = StandardScaler()\n",
    "\n",
    "features_standardized = standardizer.fit_transform(features)\n",
    "knn = KNeighborsClassifier(n_neighbors=5,n_jobs=-1)\n",
    "\n",
    "pipe = Pipeline([(\"standardizer\",standardizer),('knn',knn)])\n",
    "\n",
    "search_space = [{'knn__n_neighbors':[1,2,3,4,5,6,7,8,9,10]}]\n",
    "\n",
    "# 创建grid搜索\n",
    "classifier = GridSearchCV(\n",
    "    pipe,search_space,cv=5,verbose=0).fit(features_standardized,target)\n",
    "\n",
    "classifier.best_estimator_.get_params()['knn__n_neighbors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个基于半径的最近邻分类器\n",
    "from sklearn.neighbors import RadiusNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "standardizer = StandardScaler()\n",
    "features_standardized = standardizer.fit_transform(features)\n",
    "\n",
    "rnn = RadiusNeighborsClassifier(\n",
    "    radius=.5,n_jobs=-1).fit(features_standardized,target)\n",
    "\n",
    "new_observations = [[1,1,1,1]]\n",
    "\n",
    "rnn.predict(new_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "#逻辑回归\n",
    "#######################################\n",
    "\n",
    "# 训练二元分类器\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data[:100,:]\n",
    "target = iris.target[:100]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "logistic_regression = LogisticRegression(random_state=0)\n",
    "model = logistic_regression.fit(features_standardized,target)\n",
    "\n",
    "new_observation = [[.5,.5,.5,.5]]\n",
    "model.predict(new_observation)\n",
    "\n",
    "model.predict_proba(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练多元分类器\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "# 创建一对多的逻辑回归对象\n",
    "logistic_regression = LogisticRegression(random_state=0,multi_class=\"ovr\")\n",
    "\n",
    "model = logistic_regression.fit(features_standardized,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过正则化来减小方差\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features =  iris.data\n",
    "target = iris.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "logistic_regression = LogisticRegressionCV(\n",
    "    penalty='l2',Cs=10,random_state=0,n_jobs=-1)\n",
    "model=logistic_regression.fit(features_standardized,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在超大数据集上训练分类器\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "logistic_regression = LogisticRegression(random_state=0,solver='sag')\n",
    "model = logistic_regression.fit(features_standardized,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理不均衡的分类\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "features = features[40:,:]\n",
    "target = target[40:]\n",
    "\n",
    "target = np.where((target==0),0,1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "logistic_regression = LogisticRegression(random_state=0,class_weight='balanced')\n",
    "\n",
    "model = logistic_regression.fit(features_standardized,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# 支持向量机\n",
    "####################################\n",
    "\n",
    "# 训练一个线性分类器\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data[:100,:2]\n",
    "target = iris.target[:100]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "svc = LinearSVC(C=1.0)\n",
    "model = svc.fit(features_standardized,target)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# 画出样本点，并且根据其分类上色\n",
    "color = [\"black\" if c == 0 else \"lightgrey\" for c in target]\n",
    "plt.scatter(features_standardized[:,0],features_standardized[:,1],c=color)\n",
    "\n",
    "# 创建超平面\n",
    "w = svc.coef_[0]\n",
    "a = -w[0]/w[1]\n",
    "xx = np.linspace(-2.5,2.5)\n",
    "yy = a*xx-(svc.intercept_[0])/w[1]\n",
    "\n",
    "# 画出超平面\n",
    "plt.plot(xx,yy)\n",
    "plt.axis('off'),plt.show();\n",
    "\n",
    "#  创建一个新的样本点\n",
    "new_observation = [[-2,3]]\n",
    "\n",
    "# 预测性样本点的分类\n",
    "svc.predict(new_observation)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用核函数处理线性不可分的数据\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "# 生成两个特征\n",
    "features = np.random.randn(200,2)\n",
    "\n",
    "# 使用异域门 创建线性不可分的数据\n",
    "target_xor = np.logical_xor(features[:,0]>0,features[:,1]>0)\n",
    "target = np.where(target_xor,0,1)\n",
    "\n",
    "#  创建一个有径向基核函数的支持向量机\n",
    "svc = SVC(kernel='rbf',random_state=0,gamma=1,C=1)\n",
    "model = svc.fit(features,target)\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_decision_regions(X,y,classifier):\n",
    "    cmap = ListedColormap((\"red\",\"blue\"))\n",
    "    xx1,xx2 = np.meshgrid(np.arange(-3,3,0.02), np.arange(-3,3,0.02))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(),xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1,xx2,Z,alpha=0.1,cmap=cmap)\n",
    "    for idx,cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl,0],y=X[y==cl,1],\n",
    "                    alpha=0.8,c=cmap(idx),\n",
    "                    marker=\"+\",label=cl)\n",
    "\n",
    "# 创建一个使用线性核函数的SVC\n",
    "svc_linear = SVC(kernel='linear',random_state=0,C=1)\n",
    "svc_linear.fit(features,target)\n",
    "\n",
    "# 画出观察值和超平面\n",
    "plot_decision_regions(features,target,classifier=svc_linear)\n",
    "plt.axis('off'),plt.show();\n",
    "\n",
    "# 创建一个使用径向基核函数的SVC\n",
    "svc = SVC(kernel='rbf',random_state=0,gamma=1,C=1)\n",
    "model = svc.fit(features,target)\n",
    "\n",
    "# 画出观察值和超平面\n",
    "plot_decision_regions(features,target,classifier=svc)\n",
    "plt.axis('off'),plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算预测分类的概率\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "svc = SVC(kernel='linear',probability=True,random_state=0)\n",
    "\n",
    "model = svc.fit(features_standardized,target)\n",
    "\n",
    "new_observation = [[.4,.4,.4,.4]]\n",
    "model.predict_proba(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 识别支持向量机\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data[:100,:]\n",
    "target = iris.target[:100]\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "svc = SVC(kernel='linear',random_state=0)\n",
    "\n",
    "model = svc.fit(features_standardized,target)\n",
    "# 查看支持向量机\n",
    "model.support_vectors_\n",
    "\n",
    "# 观察值的中的索引值\n",
    "model.support_\n",
    "\n",
    "# 每个分类有几个支持向量\n",
    "model.n_support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理不均衡的分类\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data[:100,:]\n",
    "target = iris.target[:100]\n",
    "\n",
    "features = features[40:,:]\n",
    "target = target[40:]\n",
    "\n",
    "# 创建目标向量，数值0代表分类0，其他分类用数值1表示\n",
    "target = np.where((target==0),0,1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "\n",
    "svc = SVC(kernel='linear',class_weight='balanced',C=1.0,random_state=0)\n",
    "model = svc.fit(features_standardized,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 朴素贝叶斯\n",
    "#########################\n",
    "\n",
    "# 为连续的数据训练分类器\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "classifer = GaussianNB()\n",
    "\n",
    "model = classifer.fit(features,target)\n",
    "\n",
    "new_observation = [[4,4,4,0.4]]\n",
    "model.predict(new_observation)\n",
    "\n",
    "clf = GaussianNB(priors=[0.25,0.25,0.5])\n",
    "model = classifer.fit(features,target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为离散数据统计和计数数据训练分类器\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_data = np.array(['I love Brazil.Brazil!',\n",
    "                     'Brazil is best',\n",
    "                     'Germany beats both'])\n",
    "\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "features = bag_of_words.toarray()\n",
    "\n",
    "target = np.array([0,0,1])\n",
    "\n",
    "# 给定每个分类的先验概率，创建一个多项式朴素贝叶斯对象\n",
    "classifer = MultinomialNB(class_prior=[0.25,0.5])\n",
    "\n",
    "model = classifer.fit(features,target)\n",
    "\n",
    "new_observation = [[0,0,0,1,0,1,0]]\n",
    "\n",
    "model.predict(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 为具有二元特征的数据训练朴素贝叶斯分类器\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# 创建三个二元特征\n",
    "features = np.random.randint(2,size=(100,3))\n",
    "# 创建一个二元目标向量\n",
    "target = np.random.randint(2,size=(100,1)).ravel()\n",
    "# 给定每个分类的先验概率，创建一个多项式伯努利朴素贝叶斯对象\n",
    "classifer = BernoulliNB(class_prior=[0.25,0.5])\n",
    "\n",
    "model = classifer.fit(features,target)\n",
    "\n",
    "# 指定统一的先验概率\n",
    "model_uniform_prior = BernoulliNB(class_prior=None,fit_prior=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 校准预测概率\n",
    "from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "target = iris.target\n",
    "\n",
    "classifer = GaussianNB()\n",
    "classifer_sigmoid = CalibratedClassifierCV(classifer,cv=2,method='sigmoid')\n",
    "\n",
    "classifer_sigmoid.fit(features,target)\n",
    "\n",
    "new_observation = [[2.6,2.6,2.6,0.4]]\n",
    "classifer_sigmoid.predict_proba(new_observation)\n",
    "\n",
    "# 训练一个高斯朴素贝叶斯分类器来预测观察值的分类预测\n",
    "classifer.fit(features,target).predict_proba(new_observation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# 聚类\n",
    "###############################\n",
    "\n",
    "# 使用K-Means聚类算法\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "cluster = KMeans(n_clusters=3,random_state=0,n_jobs=-1)\n",
    "model = cluster.fit(features_std)\n",
    "\n",
    "# 查看预测分类\n",
    "model.labels_\n",
    "# 查看真实分类\n",
    "iris.target\n",
    "\n",
    "new_observation = [[0.8,0.8,0.8,0.8]]\n",
    "model.predict(new_observation)\n",
    "\n",
    "# 查看分类的中心点\n",
    "model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加速K-Means聚类\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "cluster = MiniBatchKMeans(n_clusters=3,random_state=0,batch_size=100)\n",
    "\n",
    "model = cluster.fit(features_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Meanshift聚类算法\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import MeanShift\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "cluster = MeanShift(n_jobs=-1)\n",
    "model = cluster.fit(features_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用DBSCAN聚类算法\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "cluster = DBSCAN(n_jobs=-1)\n",
    "model = cluster.fit(features_std)\n",
    "\n",
    "model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用层次合并聚类算法\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "features = iris.data\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_std = scaler.fit_transform(features)\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=3)\n",
    "\n",
    "model = cluster.fit(features_std)\n",
    "\n",
    "model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# 神经网络\n",
    "#########################\n",
    "\n",
    "# 为神经网络预处理数据\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "features = np.array([[-100.1,3240.1],\n",
    "                    [-200.2,-234.1],\n",
    "                    [5000.5,150.1],\n",
    "                    [6000.6,-125.1],\n",
    "                    [9000.9,-673.1]])\n",
    "\n",
    "scaler = preprocessing.StandardScaler()\n",
    "features_standardized = scaler.fit_transform(features)\n",
    "features_standardized\n",
    "\n",
    "# 打印均值和标准差\n",
    "print(\"Mean:\",round(features_standardized[:,0].mean()))\n",
    "print(\"Standard deviation:\",features_standardized[:,0].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设计一个神经网络\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# 启动神经网络\n",
    "network = models.Sequential()\n",
    "\n",
    "# 添加使用ReLU激活函数的全连接层\n",
    "network.add(layers.Dense(units=16,activation=\"relu\",input_shape=(10,)))\n",
    "\n",
    "# 添加使用ReLU激活函数的全连接层\n",
    "network.add(layers.Dense(units=16,activation='relu'))\n",
    "\n",
    "# 添加使用sigmid激活函数的全连接层\n",
    "network.add(layers.Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "# 编译神经网络\n",
    "network.compile(loss='binary_crossentropy',#交叉熵\n",
    "               optimizer='rmsprop',#均方根传播\n",
    "               metrics=['accuracy'])#将准确率作为性能指标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练一个二元分类器\n",
    "import numpy as np\n",
    "from keras.datasets import imdb #影评数据\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "\n",
    "(data_train,target_train),(data_test,target_test) = imdb.load_data(num_words=number_of_features)\n",
    "\n",
    "# 将影评数据转化为one-hot编码过的特征矩阵\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train,mode='binary')\n",
    "features_test = tokenizer.sequences_to_matrix(data_test,mode='binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units=16,activation='relu',input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16,activation='relu'))\n",
    "network.add(layers.Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy',#交叉熵\n",
    "               optimizer='rmsprop',#均方根传播\n",
    "               metrics=['accuracy'])#将准确率作为性能指标\n",
    "\n",
    "history = network.fit(features_train,#特征\n",
    "                     target_train,#目标向量\n",
    "                     epochs=3,#epoch数量\n",
    "                     verbose=1,#每个epoch之后打印描述\n",
    "                     batch_size=100,#每个批次中观察值的数量\n",
    "                     validation_data=(features_test,target_test))#测试数据\n",
    "\n",
    "features_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练一个多元分类器\n",
    "import numpy as np\n",
    "from keras.datasets import reuters\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "np.random.seed(0)\n",
    "number_of_features = 5000\n",
    "\n",
    "data = reuters.load_data(num_words=number_of_features)\n",
    "(data_train,target_vector_train),(data_test,target_vector_test) = data\n",
    "\n",
    "# 把特征数据转换成one-hot编码的特征矩阵\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train,mode='binary')\n",
    "features_test = tokenizer.sequences_to_matrix(data_test,mode='binary')\n",
    "\n",
    "# 把one-hot编码的特征向量转换成特征矩阵\n",
    "target_train = to_categorical(target_vector_train)\n",
    "target_test = to_categorical(target_vector_test)\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=100,activation='relu',input_shape=(number_of_features,)))\n",
    "\n",
    "network.add(layers.Dense(units=100,activation='relu'))\n",
    "network.add(layers.Dense(units=46,activation='softmax'))\n",
    "network.compile(loss=\"categorical_crossentropy\",#交叉熵\n",
    "               optimizer='rmsprop',#均方根传播\n",
    "               metrics=['accuracy'])#将准确率作为性能指标\n",
    "\n",
    "history = network.fit(features_train,#特征\n",
    "                     target_train,#目标向量\n",
    "                     epochs=3,#3个epoch\n",
    "                     verbose=0,#没有输出\n",
    "                     batch_size=100,#每个批次的观察值数量\n",
    "                     validation_data=(features_test,target_test))#测试数据\n",
    "\n",
    "target_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练一个回归模型\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "np.random.seed(0)\n",
    "features,target = make_regression(n_samples=10000,\n",
    "                                  n_features=3,\n",
    "                                  n_targets=1,\n",
    "                                  noise=0.0,\n",
    "                                  random_state=0)\n",
    "\n",
    "features_train,features_test,target_train,target_test = train_test_split(\n",
    "    features,target,test_size=0.33,random_state=0)\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=32,activation='relu',input_shape=(features_train.shape[1],)))\n",
    "network.add(layers.Dense(units=32,activation='relu'))\n",
    "# 添加没有激活函数的全连接层\n",
    "network.add(layers.Dense(units=1))\n",
    "\n",
    "network.compile(loss='mse',#均方误差\n",
    "               optimizer='RMSprop',#优化算法\n",
    "               metrics=['mse'])#均方误差\n",
    "\n",
    "history = network.fit(features_train,#特征\n",
    "                     target_train,#目标向量\n",
    "                     epochs=10,#epoch的数量\n",
    "                     verbose=0,#没有输出\n",
    "                     batch_size=100,#每个批次的观察值数量\n",
    "                     validation_data=(features_test,target_test))#测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 做预测\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "np.random.seed(0)\n",
    "number_of_features = 10000\n",
    "(data_train,target_train),(data_test,target_test)=imdb.load_data(num_words=number_of_features)\n",
    "\n",
    "# 把IMDB数据转换为one-hot编码的特征矩阵\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train,mode='binary')\n",
    "features_test = tokenizer.sequences_to_matrix(data_test,mode='binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16,activation='relu',input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16,activation='relu'))\n",
    "network.add(layers.Dense(units=1,activation='sigmoid'))\n",
    "network.compile(loss='binary_crossentropy',#交叉熵\n",
    "               optimizer='rmsprop',#均方根传播\n",
    "               metrics=['accuracy'])#将准确率作为性能指标\n",
    "\n",
    "history = network.fit(features_train,#特征值\n",
    "                     target_train,#目标向量\n",
    "                     epochs=3,#epoch的数量\n",
    "                     verbose=0,#没有输出\n",
    "                     batch_size=100,#每个批次的观察值数量\n",
    "                     validation_data=(features_test,target_test))#测试数据\n",
    "\n",
    "# 预测测试集的分类\n",
    "predicted_target = network.predict(features_test)\n",
    "\n",
    "# 查看第一个观察值属于分类1的预测概率\n",
    "predicted_target[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可视化训练结果历史\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "number_of_features = 10000\n",
    "\n",
    "(data_train,target_train),(data_test,target_test)=imdb.load_data(num_words=number_of_features)\n",
    "\n",
    "# 把IMDB数据转换为one-hot编码的特征矩阵\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train,mode='binary')\n",
    "features_test = tokenizer.sequences_to_matrix(data_test,mode='binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16,activation='relu',input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16,activation='relu'))\n",
    "network.add(layers.Dense(units=1,activation='sigmoid'))\n",
    "network.compile(loss='binary_crossentropy',#交叉熵\n",
    "               optimizer='rmsprop',#均方根传播\n",
    "               metrics=['accuracy'])#将准确率作为性能指标\n",
    "\n",
    "history = network.fit(features_train,#特征值\n",
    "                     target_train,#目标向量\n",
    "                     epochs=15,#epoch的数量\n",
    "                     verbose=0,#没有输出\n",
    "                     batch_size=1000,#每个批次的观察值数量\n",
    "                     validation_data=(features_test,target_test))#测试数据\n",
    "\n",
    "# 获取训练集和测试集的损失历史数值\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "# 为每一个epoch创建编号\n",
    "epoch_count = range(1,len(training_loss)+1)\n",
    "# 画出损失的历史数值\n",
    "plt.plot(epoch_count,training_loss,'r--')\n",
    "plt.plot(epoch_count,test_loss,'b-')\n",
    "plt.legend(['Training Loss','Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();\n",
    "\n",
    "# 获取训练集和测试集数据的准确率和历史数值\n",
    "training_accuracy = history.history['accuracy']\n",
    "test_accuracy = history.history['val_accuracy']\n",
    "plt.plot(epoch_count,training_accuracy,'r--')\n",
    "plt.plot(epoch_count,test_accuracy,'b--')\n",
    "\n",
    "# 可视化准确率的历史数值\n",
    "plt.legend(['Training Accuracy','Test Accuracy'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy Score')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过权重调节减少过拟合\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n",
    "\n",
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "\n",
    "(data_train,target_train),(data_test,target_test)=imdb.load_data(num_words=number_of_features)\n",
    "\n",
    "# 把IMDB数据转换为one-hot编码的特征矩阵\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train,mode='binary')\n",
    "features_test = tokenizer.sequences_to_matrix(data_test,mode='binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16,activation='relu',kernel_regularizer=regularizers.l2(0.01),\n",
    "                         input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16,kernel_regularizer=regularizers.l2(0.01),activation='relu'))\n",
    "network.add(layers.Dense(units=1,activation='sigmoid'))\n",
    "network.compile(loss='binary_crossentropy',#交叉熵\n",
    "               optimizer='rmsprop',#均方根传播\n",
    "               metrics=['accuracy'])#将准确率作为性能指标\n",
    "\n",
    "history = network.fit(features_train,#特征值\n",
    "                     target_train,#目标向量\n",
    "                     epochs=3,#epoch的数量\n",
    "                     verbose=0,#没有输出\n",
    "                     batch_size=100,#每个批次的观察值数量\n",
    "                     validation_data=(features_test,target_test))#测试数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过提前结束减少过拟合\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "\n",
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "\n",
    "(data_train,target_train),(data_test,target_test)=imdb.load_data(num_words=number_of_features)\n",
    "\n",
    "# 把IMDB数据转换为one-hot编码的特征矩阵\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train,mode='binary')\n",
    "features_test = tokenizer.sequences_to_matrix(data_test,mode='binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16,activation='relu',input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16,activation='relu'))\n",
    "network.add(layers.Dense(units=1,activation='sigmoid'))\n",
    "network.compile(loss='binary_crossentropy',#交叉熵\n",
    "               optimizer='rmsprop',#均方根传播\n",
    "               metrics=['accuracy'])#将准确率作为性能指标\n",
    "# 设置一个回调函数来提前结束训练，并保存训练结束是的最佳模型\n",
    "callbacks = [EarlyStopping(monitor='val_loss',patience=2),\n",
    "            ModelCheckpoint(filepath='best_model.h5',monitor='val_loss',\n",
    "                           save_best_only=True)]\n",
    "\n",
    "history = network.fit(features_train,#特征值\n",
    "                     target_train,#目标向量\n",
    "                     epochs=20,#epoch的数量\n",
    "                     verbose=0,#没有输出\n",
    "                     batch_size=100,#每个批次的观察值数量\n",
    "                     validation_data=(features_test,target_test))#测试数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 通过Dropout减少过拟合\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "\n",
    "(data_train,target_train),(data_test,target_test)=imdb.load_data(num_words=number_of_features)\n",
    "\n",
    "# 把IMDB数据转换为one-hot编码的特征矩阵\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train,mode='binary')\n",
    "features_test = tokenizer.sequences_to_matrix(data_test,mode='binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "# 为输入层添加一个Dropout层\n",
    "network.add(layers.Dropout(0.2,input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16,activation='relu'))\n",
    "# 为前面的隐藏层添加一个Dropout层\n",
    "network.add(layers.Dropout(0.5))\n",
    "network.add(layers.Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy',#交叉熵\n",
    "               optimizer='rmsprop',#均方根传播\n",
    "               metrics=['accuracy'])#将准确率作为性能指标\n",
    "\n",
    "history = network.fit(features_train,#特征值\n",
    "                     target_train,#目标向量\n",
    "                     epochs=3,#epoch的数量\n",
    "                     verbose=0,#没有输出\n",
    "                     batch_size=100,#每个批次的观察值数量\n",
    "                     validation_data=(features_test,target_test))#测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型训练过程\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "\n",
    "(data_train,target_train),(data_test,target_test)=imdb.load_data(num_words=number_of_features)\n",
    "\n",
    "# 把IMDB数据转换为one-hot编码的特征矩阵\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "features_train = tokenizer.sequences_to_matrix(data_train,mode='binary')\n",
    "features_test = tokenizer.sequences_to_matrix(data_test,mode='binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "network.add(layers.Dense(units=16,activation='relu',input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=16,activation='relu'))\n",
    "network.add(layers.Dense(units=1,activation='sigmoid'))\n",
    "network.compile(loss='binary_crossentropy',#交叉熵\n",
    "               optimizer='rmsprop',#均方根传播\n",
    "               metrics=['accuracy'])#将准确率作为性能指标\n",
    "\n",
    "# 设置一个回调函数来提前结束训练，并保存训练结束时的最佳模型\n",
    "checkpoint = [ModelCheckpoint(filepath='models.hdf5')]\n",
    "\n",
    "history = network.fit(features_train,#特征值\n",
    "                     target_train,#目标向量\n",
    "                     epochs=3,#epoch的数量\n",
    "                     verbose=0,#没有输出\n",
    "                     batch_size=100,#每个批次的观察值数量\n",
    "                     validation_data=(features_test,target_test))#测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用k折交叉验证评估神经网络\n",
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "np.random.seed(0)\n",
    "number_of_features = 100\n",
    "\n",
    "features,target = make_classification(n_samples=10000,\n",
    "                                     n_features=number_of_features,\n",
    "                                     n_informative=3,\n",
    "                                     n_redundant=0,\n",
    "                                     n_classes=2,\n",
    "                                     weights=[.5,.5],\n",
    "                                     random_state=0)\n",
    "\n",
    "# 创建一个函数，返回编译过的网络\n",
    "def create_network():\n",
    "    network = models.Sequential()\n",
    "    network.add(layers.Dense(units=16,activation='relu',input_shape=(number_of_features,)))\n",
    "    network.add(layers.Dense(units=16,activation='relu'))\n",
    "    network.add(layers.Dense(units=1,activation='sigmoid'))\n",
    "    network.compile(loss='binary_crossentropy',#交叉熵\n",
    "                   optimizer='rmsprop',#均方根传播\n",
    "                   metrics=['accuracy'])#将准确率作为性能指标\n",
    "    return network\n",
    "\n",
    "# 封装Keras模型，以便它能被scikit-learn使用\n",
    "neural_network = KerasClassifier(build_fn=create_network,\n",
    "                                epochs=10,\n",
    "                                batch_size=100,\n",
    "                                verbose=0)\n",
    "\n",
    "# 使用3折交叉验证来评估神经网络\n",
    "cross_val_score(neural_network,features,target,cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调校神经网络\n",
    "import numpy as np\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "np.random.seed(0)\n",
    "number_of_features = 100\n",
    "\n",
    "features,target = make_classification(n_samples=10000,\n",
    "                                     n_features=number_of_features,\n",
    "                                     n_informative=3,\n",
    "                                     n_redundant=0,\n",
    "                                     n_classes=2,\n",
    "                                     weights=[.5,.5],\n",
    "                                     random_state=0)\n",
    "\n",
    "# 创建一个函数，返回编译过的网络\n",
    "def create_network(optimizer='rmsprop'):\n",
    "    network = models.Sequential()\n",
    "    network.add(layers.Dense(units=16,activation='relu',input_shape=(number_of_features,)))\n",
    "    network.add(layers.Dense(units=16,activation='relu'))\n",
    "    network.add(layers.Dense(units=1,activation='sigmoid'))\n",
    "    network.compile(loss='binary_crossentropy',#交叉熵\n",
    "                   optimizer=optimizer,#均方根传播\n",
    "                   metrics=['accuracy'])#将准确率作为性能指标\n",
    "    return network\n",
    "\n",
    "# 封装Keras模型以便它能被scikit-learn使用\n",
    "neural_network = KerasClassifier(build_fn=create_network,verbose=0)\n",
    "# 创建超参数空间\n",
    "epochs=[5,10]\n",
    "batches=[5,10,100]\n",
    "optimizers=['rmsprop','adam']\n",
    "\n",
    "# 创建超参数选项\n",
    "hyperparameters = dict(optimizer=optimizers,epochs=epochs,batch_size=batches)\n",
    "# 创建超参数选项\n",
    "grid = GridSearchCV(estimator=neural_network,param_grid=hyperparameters)\n",
    "# 实现网格搜索\n",
    "grid_result = grid.fit(features,target)\n",
    "\n",
    "# 查看最优神经网络的超参数\n",
    "grid_result.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 可视化神经网络\n",
    "# 由于报错，故不运行\n",
    "# from keras import models\n",
    "# from keras import layers\n",
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "# from keras.utils import plot_model\n",
    "\n",
    "# network = models.Sequential()\n",
    "# network.add(layers.Dense(units=16,activation='relu',input_shape=(10,)))\n",
    "# network.add(layers.Dense(units=16,activation='relu'))\n",
    "# network.add(layers.Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "# # 可视化网络结构\n",
    "# SVG(model_to_dot(network,show_shapes=True).create(prog='dot',format='svg'))\n",
    "# # 将可视化后的网络结构图保存为文件\n",
    "# plot_model(network,show_shapes=True,to_file='network.png')\n",
    "# # 画出网络结构\n",
    "# SVC(model_to_dot(network,show_shapes=False).create(prog='dot',format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 图像分类\n",
    "# 需要修改接口，故不运行\n",
    "# import numpy as np\n",
    "# from keras.datasets import mnist\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense,Dropout,Flatten\n",
    "# from keras.layers.convolutional import Conv2D,MaxPooling2D\n",
    "# from keras.utils import np_utils\n",
    "# from keras import backend as K\n",
    "\n",
    "# # 设置彩色通道值有限\n",
    "# K.set_image_data_format('channels_first')\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "# # 图像信息\n",
    "# channels =1\n",
    "# height = 28\n",
    "# width = 28\n",
    "\n",
    "# # 从MNIST数据集中读取数据和目标\n",
    "# (data_train,target_train),(data_test,target_test) = mnist.load_data()\n",
    "# # 将训练集图像数据转换成特征\n",
    "# data_train = data_train.reshape(data_train.shape[0],channels,height,width)\n",
    "# # 将测试集图像数据转换成特征\n",
    "# data_test = data_test.reshape(data_test.shape[0],channels,height,width)\n",
    "# # 将像素的强度值收缩到0和1之间\n",
    "# features_train = data_train/255\n",
    "# features_test = data_test/255\n",
    "\n",
    "# # 对目标进行one-hot编码\n",
    "# target_train = np_utils.to_categorical(target_train)\n",
    "# target_test = np_utils.to_categorical(target_test)\n",
    "# number_of_classer = target_test.shape[1]\n",
    "\n",
    "# network = Sequential()\n",
    "# # 添加有64个过滤器、一个大小为5x5的窗口和ReLU激活函数的卷积层\n",
    "# network.add(Conv2D(filters=64,kernel_size=(5,5),input_shape=(channels,width,height),activation='relu'))\n",
    "# # 添加带一个2x2窗口的最大池化层\n",
    "# network.add(MaxPooling2D(pool_size=(2,2)))\n",
    "# # 添加Dropout层\n",
    "# network.add(Dropout(0.5))\n",
    "# # 添加一层来压平输入\n",
    "# network.add(Flatten())\n",
    "# # 添加带ReLU激活函数的有128个神经元的全连接层\n",
    "# network.add(Dense(128,activationivation='relu'))\n",
    "# # 添加Dropout层\n",
    "# network.add(Dropout(0.5))\n",
    "# # 添加使用softmax激活函数的全连接层\n",
    "# network.add(Dense(number_of_classer,activation='softmax'))\n",
    "\n",
    "# network.compile(loss='binary_crossentropy',#交叉熵\n",
    "#                optimizer=optimizer,#均方根传播\n",
    "#                metrics=['accuracy'])#将准确率作为性能指标\n",
    "\n",
    "# network.fit(features_train,#特征\n",
    "#              target_train,#目标向量\n",
    "#              epochs=2,#epoch的数量\n",
    "#              verbose=0,#没有输出\n",
    "#              batch_size=1000,#每个批次的观察值数量\n",
    "#              validation_data=(features_test,target_test))#测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 通过图像增强来改善卷积神经网络的性能\n",
    "# 运行不了\n",
    "# from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# # 创建图像增强对象\n",
    "# augmentation = ImageDataGenerator(feaurewise_center=True,#实施ZCA白化\n",
    "#                                  zoom_range=0.3,#随机放大图像\n",
    "#                                  width_shift_range=0.2,#随机打乱图像\n",
    "#                                  horizontal_flip=True,#随机翻转图像\n",
    "#                                  rotation_range=90)#随机旋转图像\n",
    "# # 对raw/images文件夹下所有的图像进行处理\n",
    "# augment_images = augmentation.flow_from_directory('row/images',#图像文件夹\n",
    "#                                                  batch_size=32,#批次的大小\n",
    "#                                                  class_mode='binary',#分类\n",
    "#                                                  save_to_dir='processed/images')\n",
    "\n",
    "# network.fit_generator(augment_images,\n",
    "#                      #在每个epoch中调用生成器的次数\n",
    "#                      steps_per_epoch=2000,\n",
    "#                      #epoch的数量\n",
    "#                      epochs=5,\n",
    "#                      #测试数据生成器\n",
    "#                      validation_data=augment_images_test,\n",
    "#                      #在每个测试epoch中调用生成器的次数\n",
    "#                      validation_steps=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本分类\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "(data_train,target_train),(data_test,target_test)=imdb.load_data(num_words=number_of_features)\n",
    "\n",
    "# 采用添加填充值或者截断的方式，使每个样本都有400个特征\n",
    "features_train = sequence.pad_sequences(data_train,maxlen=400)\n",
    "features_test = sequence.pad_sequences(data_test,maxlen=400)\n",
    "\n",
    "network = models.Sequential()\n",
    "# 加入嵌入层\n",
    "network.add(layers.Embedding(input_dim=number_of_features,output_dim=128))\n",
    "# 添加一个有128个神经元的长期记忆网络层\n",
    "network.add(layers.LSTM(units=128))\n",
    "# 添加使用simgoid激活函数的全连接层\n",
    "network.add(layers.Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy',#交叉熵\n",
    "               optimizer='rmsprop',#均方根传播\n",
    "               metrics=['accuracy'])#将准确率作为性能指标\n",
    "\n",
    "history = network.fit(features_train,#特征值\n",
    "                     target_train,#目标向量\n",
    "                     epochs=3,#epoch的数量\n",
    "                     verbose=0,#没有输出\n",
    "                     batch_size=1000,#每个批次的观察值数量\n",
    "                     validation_data=(features_test,target_test))#测试数据\n",
    "\n",
    "print(data_train[0])\n",
    "print(data_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# 保存和加载训练后的模型\n",
    "#######################\n",
    "\n",
    "# # 保存和加载一个scikit-learn模型\n",
    "# 报错\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn import datasets\n",
    "# from sklearn.externals import joblib\n",
    "\n",
    "# iris = datasets.load_iris()\n",
    "# features = iris.data\n",
    "# target = iris.target\n",
    "\n",
    "# classifer = RandomForestClassifier()\n",
    "# model = classifer.fit(features,target)\n",
    "# # 把模型保存为pickle文件\n",
    "# joblib.dump(model,'model.pkl')\n",
    "\n",
    "# # 从文件中加载模型\n",
    "# classifer = joblib.load('model.pkl')\n",
    "# new_observation = [[5.2,3.2,1.1,0.1]]\n",
    "# classifer.predict(new_observation)\n",
    "\n",
    "# import sklearn\n",
    "# # 获取scikit-learn的版本\n",
    "# scikit_version = joblib.__version__\n",
    "# # 把模型保存为pickle文件\n",
    "# joblib.dump(model,'model_{version}.pkl'.format(version=scikit_version))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存和加载Keras模型\n",
    "import numpy as np\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.models import load_model\n",
    "\n",
    "np.random.seed(0)\n",
    "number_of_features = 1000\n",
    "\n",
    "(train_data,train_target),(test_data,test_target)=imdb.load_data(num_words=number_of_features)\n",
    "\n",
    "# 把IMDB数据转换为one-hot编码的特征矩阵\n",
    "tokenizer = Tokenizer(num_words=number_of_features)\n",
    "train_features = tokenizer.sequences_to_matrix(data_train,mode='binary')\n",
    "test_features = tokenizer.sequences_to_matrix(data_test,mode='binary')\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(units=16,activation='relu',input_shape=(number_of_features,)))\n",
    "network.add(layers.Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "network.compile(loss='binary_crossentropy',#交叉熵\n",
    "               optimizer='rmsprop',#均方根传播\n",
    "               metrics=['accuracy'])#将准确率作为性能指标\n",
    "\n",
    "history = network.fit(train_features,#特征值\n",
    "                     target_train,#目标向量\n",
    "                     epochs=3,#epoch的数量\n",
    "                     verbose=0,#没有输出\n",
    "                     batch_size=100,#每个批次的观察值数量\n",
    "                     validation_data=(test_features,test_target))#测试数据\n",
    "\n",
    "# 保存神经网络\n",
    "network.save('model.h5')\n",
    "# 加载神经网络\n",
    "network=load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
